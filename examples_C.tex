%$\assert{\ulcorner \mathsf{aligned maddr} \urcorner \ast \mathsf{r14} \mapsto_{\textsf{r}} \textsf{r14v} \ast \mathsf{r13} \mapsto_{\textsf{r}} \textsf{maddr} \ast \mathsf{rdi} \mapsto_{\textsf{r}} \textsf{rdiv} \ast \mathsf{rax} \mapsto_{\textsf{r}} \textsf{raxv}}$
%$\assert{\ulcorner \ptablestore !! \textsf{maddr} = \textsf{None} \urcorner \ast \ownGhost\gammaPred{\authfull{\ptableabswalk\ptablestore}} \ast \textsf{Pf}} $
\definecolor{main-color}{rgb}{0.6627, 0.7176, 0.7764}
\definecolor{back-color}{rgb}{0.1686, 0.1686, 0.1686}
\definecolor{string-color}{rgb}{0.3333, 0.5254, 0.345}
\definecolor{key-color}{rgb}{0.8, 0.47, 0.196}

\newcommand{\sumwalkabsent}{
  \ownGhost\gammaPred{\authfrag{\singletonMap{\texttt{entry+KERNBASE}}{(\textsf{qfrac}, \textsf{entry})}}}
}


\newcommand{\ventry}{\texttt{entry + KERNBASE}}
\newcommand{\entry}{\texttt{entry}}
\newcommand{\qfraczero}{\textsf{qfrac}}
\newcommand{\true}{\textsf{true}}
\tikzstyle{boxedassert_border} = [sharp corners,line width=0.2pt]
\NewDocumentCommand \boxedassertpv {O{} m o}{%
	\tikz[baseline=(m.base)]{
		%	  \node[rectangle, draw,inner sep=0.8pt,anchor=base,#1] (m) {${#2}\mathstrut$};
		\node[rectangle,inner sep=1.5pt,outer sep=0.2pt,anchor=base] (m) {${\,#2\,}\mathstrut$};
		\draw[#1,boxedassert_border] ($(m.south west)$) rectangle ($(m.north east)$);
	}\IfNoValueF{#3}{^{\,#3}}%
}
\newcommand*{\knowInvpv}[2]{\boxedassertpv{#2}[#1]}
\newcommand*{\ownGhostpv}[2]{\boxedassertpv[dash dot]{#2}[#1]}

\newcommand{\sumpv}[3]{
  \ownGhostpv\gammaPred{\authfrag{\singletonMap{#1}{(#2, #3)}}}
}

\newcommand{\pvmapping}[1]{\mathcal{A}\textsf{PVMappings}(#1)}


\newcommand{\fpaddr}{\texttt{fpaddr}}
\newcommand{\specline}[1]{{\color{blue}\left\{#1\right\}}}
\newcommand{\sumapacesfull}[2]{
  \ownGhost\gammaPreds{\authfull{\singletonMap{#1}{#2}}}
}
\section{Experiments}
\label{sec:experiment}
%To both validate and demonstrate the value of the modal approach to reasoning about virtual memory management, 
% we study several
% We validate our logic by studying
% distillations of key VMM functionality.
% real concerns of virtual memory managers.
% Recall from Section \ref{sec:logic} that virtual points-to assertions work just like regular points-to assertions, by design.
In this section, we verify several critical and challenging pieces of VMM code.
First, in several stages, we work up to mapping a new page in the current address space.
This requires a number of independently challenging substeps: dynamically traversing a page table to find
the appropriate L1 entry to update; inserting additional levels of the page table if necessary (updating
the VMM invariants along the way);
converting the physical addresses found in intermediate entries into the corresponding virtual addresses
that can be used for memory access;
installing the new mapping;
and collecting sufficient resources to form a virtual points-to assertion.
Of these, only the second-to-last step (installing the correct mapping into the
current address space) has previously been formally verified with respect to a machine model with address translation.
Second, we formally verify a switch into a new address space as part of a task switch,
the first such verification handling both old and new processes' assertions (in different address spaces) at the time of the switch.

%\begin{comment}
%\todo[inline]{Identity mappings are difficult, and our current approach won't quite work. Consider trying to have a virtual pointsto for an actual page table entry (i.e., that one could use to update a page table mapping), while also having a virtual pointsto for an address that entry mapped. With the current (let's call it v1) solution, we can't actually have both of those simultaneously!  That's because the PTE pointsto will assert full ownership of the physical memory cell holding the PTE as its data value, while the virtual pointsto for the data mapped by that entry will \emph{also} assert (fractional) ownership of all entries a page table walk would traverse.
%}
%\todo[inline,color=violet]{This doesn't seem to cause issues with the mapping/unmapping examples, only with changing intermediate page table pointers. The mapping example requires a virtual pointsto for the blank PTE, and once filled in that ownership can be immediately split to create the 512 new virtual pointsto assertions for the newly mapped page. Conversely, for unmapping we'd assume ownership of all the relevant virtual pointsto assertions for the page we're unmapping, at which point we can (with a bit of work) show that they all correspond to the same L1 PTE, and extract the 512 fractional shares of that entry from the pointsto assertions.  But changing intermediate page tables, as one would do for coallescing or splitting a superpage while preserving the virtual-to-physical mappings, couldn't be done without some really complicated separating implication tricks.}
%\todo[inline,color=green]{One possible approach to resolving this, which we came up with in our Tuesday meeting, is to recognize that the current (v1) virtual points-to is too strong, because it really doesn't care about \emph{owning} those fractional resources, it only cares that \emph{something} ensures the correct page table walk exists. Iris has a ghost map resource where authoritative ownership of an individual key-value pair can be handled as a resource.  (Colin was using this in the filesystem cache.)
%We can use that mechanism to separate the virtual-to-physical translation from the physical memory involved (Kolanski and Klein may have done something similar for different reasons): (fractional) virtual points-to assertions can be defined in terms of (fractional) ownership of these authoritative ghost map entry assertions, plus sharing an invariant that the current installed page table respects all entries of the mapping. Unmapping collects the authoritative map kvpairs from collecting the assertions, and then can remove them from the ghost map and update the page tables. Critically, physical ownership of the page tables then lives in the invariant on the current page table, so some virtual pointsto assertions can refer to memory in those page tables.
%This still works with the modality, since that invariant is also semantically a predicate on a page table root.
%Let's call this v2.
%}
%\end{comment}
\subsection{Traversing Live Page Tables}
\label{sec:traversing}
We build up to the main task of mapping a new page after traversing the page tables in the software.
This algorithm is complex and corresponds to a significant amount of assembly code.
To assist with readability, we present C code for this process, with assertions adjusted slightly to refer to
C program variables rather than registers. The actual verification was carried out on x86-64 assembly
\emph{generated from this source code}.
Listings of the assembly fragments with inline assertions appear in Appendix ???. %\ref{apdx:asm}.
\todo[inline]{Need to add the assembly appendix.}
Whether in C or assembly,
the page table traversal involved in mapping a new page is very challenging functionality to verify.
Loading the current table root from \lstinline|cr3| is straightforward (a \lstinline|mov| instruction).
However, this produces the \emph{physical} address stored in \lstinline|cr3|, not a \emph{virtual} address the kernel code can use to access that memory.
This problem repeats at each level of the page table: assuming that the code has \emph{somehow} read the appropriate L4 (or L3, or L2) entry, those entries again
yield physical addresses, not virtual.
The only prior work to verify page mapping ignored the traversal and only verified mapping
assuming code \emph{already} had an appropriate virtual address for the L1 entry, where a physical
address could simply be stored. Our proof is the first to additionally deal with the critical code
leading up to that point.

\paragraph{Code Overview}
As described in Section \ref{sec:background}, mapping a new page consists of 
simulating the hardware address translation of Figure \ref{fig:pagetables}, but in software.
The code for this task takes three explicit parameters:
the root pointer (read from \lstinline|cr3| by earlier code),
the page-aligned \emph{virtual} address (\lstinline|va|) at which to make a new piece of memory accessible,
and the \emph{physical} address (\lstinline|fpaddr|) of the memory to map in that location.
The function we ultimately verify, \lstinline|vaspace_mappage| (Figure \ref{fig:mapping_codeC}),
relies primarily on a helper function already shown in Figure \ref{fig:pagetablescode}.
\lstinline|walkpgdir| finds the (virtual) address of the the correct L1 entry to translate \lstinline|va|,
by walking the page tables in software one level at a time.
\lstinline|vaspace_mappage| then uses the result to install the new entry.
\lstinline|walkpgdir| itself relies on its own helper function \lstinline|pte_get_next_table|, also shown in Figure \ref{fig:pagetablescode},
which implements a single-level of traversal from level $n+1$ to level $n$ (and whose specification and proof are therefore
parameterized by page table level), allocating additional levels as needed.

We organize our explanation of the proofs by essentially following execution from the start of \lstinline|walkpgdir|, through
execution of \lstinline|pte_get_next_table|, and out to its callsite in \lstinline|vaspace_mappage|.
While slightly awkward because we start in the middle of the mapping execution,
this ordering allows us to start with the simpler pieces of the proof, and incrementally explain the complexities
of the proofs and kernel invariant, before concluding with the top-level verification.


% The mapping operation of Figure \ref{fig:mapping_codeC} assumes an operation \textsf{walkpgdir} which must traverse the page tables
% in order to locate the address of the L1 entry to update --- 
% % possibly allocating tables for levels 3, 2, and 1 in the process,
% % installing them into levels 4, 3, and 2, along the way.
% possibly allocating new L3, L2, and L1 tables as necessary.

\todo[inline]{Colin WIP note: The organization is odd here.
We talk through access to entry, then start talking about identity mappings even though we don't need that info
until the P2V call later. But we do need the part of the extended invariant that deals with the
entry-present qfrac bit, which does only hold for those addresses used for page tables, so is part
of the same invariant. Maybe we should reorg to first talk through the logic, 
calling out the two problems the new invariant solves (permission to update the entry and p2v) along the way,
then have a section called "extended address space invariant" or similar to talk about the solutions.
This would roughly mean moving 5.1.3 (installing a new table) up
}
\todo[inline]{this and other figures need assertion updates to talk about C: see entry\_val (needs adjustment to points-to preconditions
and explanation of entry pointing to a C bitfield so entry\_val encompasses the various bits), nextpaddr
(was logical name in asm proof, but not a C variable)
}
\todo[inline]{need to narrate the main parts of assertions in prose}


\begin{figure}\footnotesize
\ifPLDI
\begin{lstlisting}[language=C,mathescape,escapeinside={(*}{*)}]
/* @param entry: virtual address of level n+1 entry which should point to the base of a level n table
   @return next: a virtual pointer to the base of a valid level n table */
pte_t *pte_get_next_table(pte_t *entry) {
  pte_t *next;
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{entry}\},m) \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$(*\label{line:precondition_entry_out}*)
  $\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})\urcorner}_{\rtv}$ (*\label{line:get_next_vpte_preconditionC}*)
  $\specline{\ulcorner\textsf{entry\_present(entry\_val)}\urcorner \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(entry\_val.pfn) + i * 8}}{\textsf{v-1}} }_{\rtv} $(*\label{line:precondition_conditional}*) 
  /* Reading the page table is entry justified by the virtual pte-points-to */
  if (!entry->present(*\label{line:read_entry_contentsC}*)(*\label{line:conditional_childrenC}*)(*\label{line:check_entry_present_jumpC}*)) {// If the present bit is zero, need to allocate next level (* \label{line:check_entry_presentC} *)(* \label{line:mask_presentC} *) 
    (*\label{line:refined_fractional_ownership}*)(*\label{line:pass_addrof_nextC}*)(*\label{line:alloc_path_startC}*)$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present entry\_val})  \urcorner }_{\rtv}$ (*\label{line:after_concluding_qfrac1C}*)
    pte_initialize(entry); // Allocate a zeroed physical page and install in entry(*\label{line:call_to_pte_initializeC}\label{line:page_of_capsC}*)
    $\specline{ \ldots \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present entry\_val})  \urcorner \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present (pfn\_set (entry\_val nextpaddr))}  \urcorner   }_{\rtv}$
    $\specline{ \begin{array}{l}\textsf{entry\_present (pte\_initialized (pfn\_set(entryv nextpaddr)))} \wand  \\ \;\;\;\;\;\;\;\;\  \forall_{i\in \textsf{0 ... 511} } \ldotp  \ghostmaptoken{\textsf{id}}{((\textsf{table\_root (pte\_initialized (pfn\_set(entry\_val nextpaddr)))}) + \textsf{i * 8})}{\textsf{v-1}} \end{array}  }$ (*\label{line:page_of_capsC}*)
    entry->pfn = nextpaddr; entry->present = 1; // Install newly-allocated level n table, mark present(*\label{line:install_new_entryC}*)
    /*Now we know that entry is initialized, so we satisfy the condition to access children list */(*\label{line:now_we_know}*)
    $\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}}  \textsf{pte\_initialized(pfn\_set(entryv nextpaddr))} \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present entry\_val})  \urcorner }_{\rtv}$
    $\specline{\forall_{i\in \textsf{0 ... 511} } \ldotp  \ghostmaptoken{\textsf{id}}{((\textsf{table\_root (pte\_initialized (pfn\_set(entry\_val nextpaddr)))}) + \textsf{i * 8})}{\textsf{v-1}}  }$(*\label{line:available_child_tokens}*) (*\label{line:alloc_path_endC}*)
  } (*\label{line:end_of_allocation_pathC}*)
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \}),m) \ast \textsf{entry} \mapsto_{\textsf{id}} \textsf{\_} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
  $\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{(pte\_initialized (entry\_val.pfn))} \urcorner }_{\rtv}$
  $\specline{\forall_{i\in \textsf{0 ... 511} } \ldotp  \ghostmaptoken{\textsf{id}}{((\textsf{table\_root (pte\_initialized (entry\_val.pfn)))}) + \textsf{i * 8})}{\textsf{v-1}}  }$ (*\label{line:childrenC}*)
  uintptr_t next_phys_addr = PTE_PFN_TO_ADDR(entry->pfn); // Fetch physical address of next table (*\label{line:finalpieceS}*)(*\label{line:extract_pfn}*)
  uintptr_t next_virt_addr = (uintptr_t) P2V(next_phys_addr); // Convert to virtual address (*\label{line:p2vC}*)
  next = (pte_t *) next_virt_addr;(*\label{line:finalpieceE}*)
  return next;
}
\end{lstlisting}
\else
\begin{lstlisting}[mathescape, basicstyle=\footnotesize,escapeinside={(*}{*)}]
;;pte_t *pte_get_next_table(pte_t *entry) {
... ;; setting up the stack
;; pte_t *next;
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{entry}\},m)  \ast \texttt{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast  \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{\_} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{\_} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})\urcorner}_{\rtv}$ (*\label{line:get_next_vpte_precondition}*)
$\specline{\ulcorner\textsf{entry\_present(entry\_val)}\urcorner \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(entry\_val.pfn) + i * 8}}{\textsf{v-1}} }_{\rtv} $ (*\label{line:conditional_children}*)
mov    -0x8[rbp],rdi
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{entry\},m)   \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{\_}  \ast  \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{entry} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})\urcorner}_{\rtv}$
$\specline{\ulcorner\textsf{entry\_present(entry\_val)}\urcorner \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(entry\_val.pfn) + i * 8}}{\textsf{v-1}} }_{\rtv} $
mov     rdi, r8
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry}  \ast  \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{entry} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})\urcorner}_{\rtv}$
$\specline{\ulcorner\textsf{entry\_present(entry\_val)}\urcorner \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(entry\_val.pfn) + i * 8}}{\textsf{v-1}} }_{\rtv} $
mov    [r8],rdi (*\label{line:read_entry_contents}*)
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry}  \ast  \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{entry\_val}  \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})  \urcorner }_{\rtv}$
$\specline{\ulcorner\textsf{entry\_present(entry\_val)}\urcorner \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(entry\_val.pfn) + i * 8}}{\textsf{v-1}} }_{\rtv} $
and    0x1,rdi (* \label{line:mask_present} *)
mov    rdi,rax
cmp    0x0,rax ;;  if (!entry->present) {(* \label{line:check_entry_present} *)
jne    161 <pte_get_next_table+0xa1> (* \label{line:check_entry_present_jump} *) ;; Jump if the present bit is not zero
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry} \ast \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{entry\_val \& 0x1} }_{\rtv}$
$\specline{  \textsf{entry} \mapsto_{\textsf{id}} \textsf{v} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  \ast \texttt{rax}  \mapsto_{\textsf{r}} \textsf{entry\_val \& 0x1} \ast \ulcorner\textsf{entry\_val \& 0x1}=\textsf{0x0}\urcorner}_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow  \lnot(\textsf{entry\_val \& 0x1}=\textsf{0x1})  \urcorner }_{\rtv}$ (*\label{line:before_concluding_qfrac1}*)
mov    rbp,rdi (*\label{line:alloc_path_start}*)
sub    0x10, rdi ;; Store the value of rbp minus 16 bytes (address of (*\textsf{next}*)) into rdi (*\label{line:pass_addrof_next}*)
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry} \ast \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{rbp - 16} }_{\rtv}$
$\specline{  \textsf{entry} \mapsto_{\textsf{id}} \textsf{v} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta} }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present entry\_val})  \urcorner }_{\rtv}$ (*\label{line:after_concluding_qfrac1}*)
callq  70 <pte_initialize> ;;pte_initialize(entry);(* \label{line:call_to_pte_initialize} *)
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry}  }_{\rtv}$
$\specline{  \textsf{entry} \mapsto_{\textsf{id}} \textsf{v} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta} }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present entry\_val})  \urcorner }_{\rtv}$
$\specline{\ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present (pfn\_set (entry\_val nextpaddr))}  \urcorner   }_{\rtv}$
$\specline{ \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{pfn\_set(entry\_val nextpaddr)}   }_{\rtv}$
$\specline{ \begin{array}{l}\textsf{entry\_present (pte\_initialized (pfn\_set(entryv nextpaddr)))} \wand  \\ \;\;\;\;\;\;\;\;\  \forall_{i\in \textsf{0 ... 511} } \ldotp  \ghostmaptoken{\textsf{id}}{((\textsf{table\_root (pte\_initialized (pfn\_set(entry\_val nextpaddr)))}) + \textsf{i * 8})}{\textsf{v-1}} \end{array}  }$ (*\label{line:page_of_caps}*)
... ;;entry value updates: entry->pfn = nextpaddr; entry->present = 1; (*\label{line:install_new_entry}*)
... ;;now we know that entry is initialized, so we satisfy the condition to access children list
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry}  }_{\rtv}$
$\specline{  \textsf{entry} \mapsto_{\textsf{id}} \textsf{v} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta} }_{\rtv}$
$\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}}  \textsf{pte\_initialized(pfn\_set(entryv nextpaddr))} \ast \ulcorner  \textsf{qfrac} = 1 \land \lnot(\textsf{entry\_present entry\_val})  \urcorner }_{\rtv}$
$\specline{ \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{pte\_initialized(pfn\_set(entry\_val nextpaddr))}  \ast rax \mapsto_{r} \textsf{table\_root (pte\_initialized (pfn\_set(entry\_val nextpaddr)))}  }_{\rtv}$
$\specline{\forall_{i\in \textsf{0 ... 511} } \ldotp  \ghostmaptoken{\textsf{id}}{((\textsf{table\_root (pte\_initialized (pfn\_set(entry\_val nextpaddr)))}) + \textsf{i * 8})}{\textsf{v-1}}  }$ (*\label{line:alloc_path_end}*)
;;} (*\label{line:end_of_allocation_path}*)
... ;; Code after conditional continued in Figure (*\ref{fig:p2v}*)
\end{lstlisting}
\fi
% ;;uintptr_t next_virt_addr = (uintptr_t) P2V(next_phys_addr);
% movabs KERNBASE,rcx
% add    rcx,rax
% ...
% ;;next = (pte_t *) next_virt_addr;
% ;;clean up the stack and return next
% \end{lstlisting}
\todo[inline]{this figure has an issue with entry, which is used both as a logical variable and a program variable (e.g., assertion on line 6).
I would also expect to see a more prominent use of \textsf{v} in the precondition
}
\vspace{-1em}
\caption{Ensuring \textsf{entry} points to a valid next-level table, allocating if necessary, returning its virtual address.}
\label{fig:calltopteinitializeC}
\vspace{-1em}
\end{figure}

\subsubsection{From L$n+1$ Entries to L$n$ Tables}
We will discuss access to the level 4 table later (Section \ref{wlkpgdirC}). However, for subsequent levels, the base address of level $n$ must be
fetched from the appropriate entry in the table of level $n+1$.
This is the role of \lstinline|pte_get_next_table| (originally Figure \ref{fig:pagetablescode}, with proof details in Figure \ref{fig:calltopteinitializeC}).
It is passed the virtual address of the page table entry in level $n+1$, and should return the \emph{virtual} 
address of the \emph{base} of the level $n$ table
indicated by that entry.
If the entry is empty (i.e., this is a sparse part of the page table representation),
the code also allocates a page for the level $n$ table, installs it in the level $n+1$ entry, and establishes appropriate invariants.
Figure \ref{fig:calltopteinitializeC} presents the function with proof annotations that we will explain shortly, but we first explain the functionality.
\lstinline|pte_get_next_table| accepts a \emph{virtual} address \lstinline|entry| which points to a level-$n+1$
table entry. On Line \ref{line:read_entry_contentsC}, the code checks the present bit of the entry.
If the bit is unset, there is no level-$n$ table, so one must be allocated via \lstinline|pte_initialize| (explained shortly\footnote{
  Briefly, it allocates a fresh physical page, and initializes the memory pointed to by \lstinline|entry| with that physical address.
}) and marked present.
By Line \ref{line:finalpieceS} the entry is known to be valid and contain the physical address of
the base of a level $n$ table. That address is then extracted (Line \ref{line:extract_pfn}),
converted to a virtual address (Line \ref{line:p2vC}), and returned to the caller.
We can now discuss \lstinline|pte_get_next_table|'s proof of correctness.
While at first glance this code does not sound terribly complex, beyond needing some care to distinguish physical and virtual addresses,
it has a highly nontrivial correctness argument, which depends critically on detailed invariants on how access to page table
entries is shared between parts of the kernel. No prior work has engaged with this problem.


\subsubsection{Address Space Invariant: Identity Mappings and Conditional Ownership Page Table Ownership}
\label{subsec:identitymappingsC}
Assembly-level verification of compiler output from Figure \ref{fig:calltopteinitializeC} is verbose but doable
Section \ref{sec:logic}'s logic, but only after resolving two critical challenges.\todo{Ismail, not sure if this ``verbose but doable''
 is a good framing, we should discuss
}
Two key challenges stand out and end up affecting both the pre- and post-conditions, neither of which has been addressed by prior work.
First, the update to the memory at (virtual) address \lstinline|entry| depends on subtle ownership invariants:
if the entry is present then its fractional ownership is shared with a large number of \lstinline|L4_L1_PointsTo| assertions
from the address space invariant (Figure \ref{fig:peraspaceinvariant}),
but if the entry is absent the proof requires full ownership to update it. We resolve this by extending the address space invariant
to make the owned fraction of the entry's memory \emph{dependent on its own contents}.
Second, the conversion of physical addresses into a corresponding virtual address that can be used to modify the specific
physical location relies on subtle, never-before-formalized kernel invariants.
% \looseness=-1
%
% These two factors percolate to the precondition (for conditional fractional ownership) and postcondition
% (for physical-to-virtual conversions), as the caller essentially passes output from one call to \lstinline|pte_get_next_table|
% as input to a subsequent call to traverse multiple levels.
Since the key to solving these challenges is to extend the address space invariant, we
first discuss that invariant and the kernel designs it supports, before returning to the subtle details of
verifying lines \ref{line:install_new_entryC} and \ref{line:p2vC}.

\paragraph{Physical-to-Virtual Mappings}
Kernels need to convert between physical and virtual addresses, in both directions.
Traversing the page tables in software is the simplest way to convert a virtual address to a physical address;
this is the context we are working up to.
However, implementing this virtual-to-physical (V2P) translation in software ironically requires physical-to-virtual (P2V) translation,
because the addresses stored in page table entries are physical, but memory accesses issued by the OS code use virtual addresses.
Because VMM operations are performance-critical for many workloads, most kernels
maintain invariants that enable very fast P2V conversions (rather than adding another data structure).
Specifically, many kernels maintain an invariant on their page tables that the virtual address of any page used for a page table 
% lives at a virtual address whose value 
is \emph{a constant offset from the physical address} --- a practice sometimes referred to as \emph{identity mapping} 
(even though the physical-to-virtual translation
is typically not literally the identity function, but adding a nonzero constant offset).\footnote{Some kernels do this for all physical memory on the machine, simplifying interaction
with DMA devices.
On newer platforms like RISC-V, this sometimes truly is an identity mapping ---
x86-64 machines are forced into offsets by backward compatibility with bootloaders that cannot access the full memory space of the
machine.
}
Thus \lstinline|P2V| on line \ref{line:p2vC} of Figure \ref{fig:calltopteinitializeC} is a macro for adding the fixed constant \lstinline|KERNBASE|.

Figure \ref{fig:peraspaceinvariant_with_p2v_extensionC} extends the per-address-space invariant  to also track which
addresses we can perform a P2V conversion on by adding a constant offset (i.e., the set of physical addresses which participate in page tables).
$\Xi$ is another ghost map, from physical addresses to the level of the page table they represent (1--4).
\emph{Only} physical addresses in $\Xi$ can undergo P2V conversion. 
Section \ref{sec:p2vC} describes the verification of an actual conversion,
but this invariant must be \emph{established} when adding a new page table level (notably on Line \ref{line:call_to_pte_initializeC},
hence the comment of Line \ref{line:now_we_know}).

\begin{figure*}
\footnotesize
\[
\begin{array}{l}
  \mathcal{I}\textsf{ASpace}_{\textsf{id}}(\ptablestore,\Xi,m)\stackrel{\triangle}{=} \textsf{ASpace\_Lookup}_{\textsf{id}}(\ptablestore,\Xi,m) \ast \mathsf{GhostMap}(\mathsf{id},\Xi)\ast\\
  \left(\bigast{(\vaddr, \textsf{paddr})\in \ptablestore}{\exists\;(\textsf{l4e, l3e, l2e, l1e, paddr})\ldotp \textsf{L}_{4}\_\textsf{L}_{1}\_\textsf{PointsTo}(\vaddr\textsf{, l4e, l3e, l2e, l1e, paddr})}\right)\ast \\
  \bigast{(\paddr,\mathsf{level}) \in \Xi}{\exists\; (\textsf{qfrac, q, val,}\vaddr) \ldotp \ulcorner \vaddr = \paddr + \textsf{KERNBASE} \; \textsf{level} > 1\urcorner \ast  \underbrace{\fracghostmaptoken{\delta}{\vaddr}{\paddr}{\qfrac} }_\text{\textcircled{1} Ghost translation} \ast \underbrace{\paddr \mapsto_{\mathsf{p}}\{\textsf{qfrac}\}\; \vale}_\text{\textcircled{2} Physical location}} \ast\\
   \qquad\underbrace{ \ulcorner \textsf{qfrac} = 1 \leftrightarrow \; \lnot\textsf{entry\_present }(\vale) \urcorner}_\text{\textcircled{3} Entry validity}\ast 
    \underbrace{\left(\ulcorner\textsf{present\_L}(\vale,\mathsf{level})\urcorner \wand \forall_{\textsf{i}\in\textsf{0..511}} \ldotp \ghostmaptoken{\textsf{id}}{((\mathsf{entry\_page}\;\vale) + \textsf{i * 8})}{\textsf{level-1}}\right)}_{\text{\textcircled{4} Indexing into next level of tables}} \\
  \textsf{ where } \\
   \textsf{ASpace\_Lookup}_{\textsf{id}}(\ptablestore,\Xi,m) \stackrel{\triangle}{=} \lambda\textsf{ cr3val} \ldotp \; \exists \gammaPred \; \ldotp \ulcorner m \; !!\; \textsf{cr3val} = \textsf{Some } \gammaPred \urcorner \ast
   % \ownGhost\gammaPred{\authfull{\ptableabswalk\ptablestore}} \ast  \ownGhostpv\gammaPred{\authfull{\pvmapping\Xi}}
   \ptableabswalk{\delta,\ptablestore} \ast \pvmapping{\delta,\Xi}\\
  \textsf{present\_L}(\vale,\mathsf{level})\stackrel{\triangle}{=} \mathsf{entry\_present}(\vale)\land \mathsf{level} > 0
  
\end{array}
\]
\todo[inline]{Ismail, what's APVMappings?}
\vspace{-1em}
\caption{Address space invariant of Figure \ref{fig:peraspaceinvariant} extended with a ghost map bookkeeping identity mappings. }
  \label{fig:peraspaceinvariant_with_p2v_extensionC}
\vspace{-1em}
\end{figure*}

For each $\paddr\mapsto \textsf{v} \in\Xi$, the invariant contains a virtual points-to justifying that virtual address
$\paddr+\textsf{KERNBASE}$ maps to physical address $\paddr$
(\textcircled{1} in Figure \ref{fig:peraspaceinvariant_with_p2v_extensionC});
fractional ownership of the physical memory for that page table entry (\textcircled{2}, which together with \textcircled{1} is equivalent
to a virtual points-to);
and for valid entries (with the present bit set) above L1, ghost map tokens for $\Xi$ for every entry in the table pointed to by the entry, which can be used
to repeat the process one level down (\textcircled{4}). 
% (L1 entries point to data pages, whose physical memory ownership resides in some virtual points-to).
\textcircled{4} becomes part of the precondition to \lstinline|pte_get_next_table|:
Line \ref{line:precondition_conditional}) says that if the entry is valid (points to a next-level table)
then there are tokens for accessing $\Xi$ for every entry in the next-level table.
By Line \ref{line:finalpieceS} the entry is guaranteed to be valid so all tokens for converting the next table level's physical addresses to virtual
are available (Line \ref{line:available_child_tokens}).
\looseness=-1

\paragraph{Self-Conditional Fractional Ownership}
The fractional ownership of the entry's physical memory is subtle.
As noted above, a \emph{valid} entry must coexist with the fractional ownership from
$\textsf{L}_{4}\_\textsf{L}_{1}\_\textsf{PointsTo}$ and therefore have less than full ownership,
but in the case where the entry is \emph{invalid}, Line \ref{line:call_to_pte_initializeC} must have full permissions in order
to populate the entry (i.e., to install a reference to a next-level table).
Fortunately, the entry can only be in use if its valid bit is set; if the valid bit is not set, we know
that no virtual points-to entry in $\delta$ or $\theta$ holds any partial ownership.
But determining this requires reading the very memory whose ownership is being determined.
We use the invariant portion annotated as ``Entry validity'' (\textcircled{3}) in Figure \ref{fig:peraspaceinvariant_with_p2v_extensionC} to capture this:
if the entry is invalid the invariant holds full ownership of the entry, so it can be updated;
while if the entry is valid, the invariant owns only a constant nonzero fraction sufficient to read but not modify the entry.
% \begin{equation*}
%  \ulcorner \textsf{qfrac} = 1 \leftrightarrow \; \lnot\textsf{entry\_present }(\vale) \urcorner \tag{*}
% \end{equation*}
Since the fractional ownership is always non-zero, Line \ref{line:read_entry_contentsC} in Figure \ref{fig:calltopteinitializeC} can read the entry,
and if the entry is dynamically found to be invalid, the invariant is refined (Line \ref{line:refined_fractional_ownership}) for full
ownership, allowing updates.

Note that the caller is responsible for providing this conditional ownership, having pulled it out of the invariant earlier.
This is why the precondition (Line \ref{line:preconditoin_entry_out}) explicitly excludes the entry's physical address from the invariant ($\Xi\setminus\{\mathsf{entry}\}$) ---
its relevant assertions have already been borrowed by the caller.
\looseness=-1


% Concretely speaking, going back to Line 15 in Figure \ref{fig:calltopteinitialize}, to read the value referenced by physical address \textsf{entry} while preserving the soundness of memory mappings, our extended invariant introduces the side condition (*)
% \begin{equation*}
%  \ulcorner \textsf{qfrac} = 1 \leftrightarrow \; \lnot\textsf{entry\_present }(\vale) \urcorner \tag{*}
% \end{equation*}
% assuring that looking the identity mapping for \textsf{entry} is safe under the subtle justification which equates the full ownership to the non/presence of the entry which can only be known when investigated in Line 21 in Figure \ref{fig:calltopteinitialize}.
\begin{comment}
 \begin{figure*}
   \footnotesize
   \[
 \begin{array}{l}
   \specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry} \textsf{\_} \ast \ulcorner \Xi !! \textsf{entry} = \textsf{Some V} \urcorner  }_{\rtv}  \\
\sqsubseteq \\
   \specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry}  \ast  \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{entry} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv} \\
\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})\urcorner}_{\rtv} \\
\mathsf{mov} \;   [ \mathsf{r8} ],\mathsf{rdi}\\
 \specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \},m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{entry}  \ast  \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{next} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{entry} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv} \\
\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry entry\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present entry\_val})\urcorner}_{\rtv} \\
 \end{array}
 \]
 \vspace{-1em}
\caption{Peel-out the resources needed for the virtual-pte pointsto relation for physical address \textsf{entry} to be accessed in Line 7 in Figure \ref{fig:calltopteinitializeC}. }
  \label{fig:lookingupidpamppingC}
 \end{figure*}
\end{comment}


 \subsubsection{Installing a New Table}
\todo[inline]{C edit frontier. Should see if we can condense this discussion, since the C is only 2 lines and mostly follows from assumptions
about kalloc. Maybe this should get inlined into the Self-Conditional Fractional Ownership above, and likewise the P2V subsubsection below into the Physical-to-Virtual Mappings
bit above. Yeah, they should. I think the expository order still makes sense even if it's not program order; maybe explain we treat the
success case first and the backup case second? This would then still permit the discussion of pte initialize to be near the proof of its call}
 After obtaining the identity mapping for \textsf{entry}, we are able to load the \textsf{entry\_val} into \textsf{rdi}, and check the presence bit through
\ifPLDI
Line \ref{line:mask_presentC} % in condensed version, all on same line
\else
Lines \ref{line:mask_present}--\ref{line:check_entry_present} 
\fi
in Figure \ref{fig:calltopteinitializeC}.
Accessing the presence bit and checking the value allows us to exploit the condition (*) that was just discussed when verifying the allocation
path (i.e., when the entry is invalid and Lines \ref{line:alloc_path_startC}--\ref{line:alloc_path_endC} in Figure \ref{fig:calltopteinitializeC}
must allocate the next level of tables).
This operation is subtle. To reiterate: the operation requires that the relevant table entry is readable, but the exact portion of ownership
returned must be determined by inspecting the valid bit of the value in memory --- so full ownership is returned only for unused entries.
When the bit is not set, that entails full ownership of the entry's memory ($\textsf{qfrac} = 1$) and justifies writing to that memory.
Otherwise, the code jumps past the end of this listing, to the following code at the top of Figure \ref{fig:p2v} (which is also the
continuation of this code).

\begin{figure}\footnotesize
  \begin{lstlisting}[mathescape,escapeinside={(*}{*)}]
/* @param entry: virtual address of a non-present entry to initialize with a new physical page */
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \lnot(\textsf{entry\_present entry\_val} }_{\rtv}$
$\specline{\textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,1}} \textsf{entry entryv} \ast \ghostmaptoken{\textsf{id}}{(\mathsf{entry})}{\textsf{level}} }$
void pte_initialize(pte_t *entry) {
  pte_t *local = kalloc(); // Allocate a full zeroed page for 512 8-byte entries(*\label{line:call_to_kallocC}*)
  entry->pfn = PTE_ADDR_TO_PFN((uintptr_t) local);
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \lnot(\textsf{entry\_present entry\_val}) }$
  $\specline{ \textsf{entry+KERNBASE} \mapsto_{\mathsf{vpte,1}} \mathsf{entry} \; \textsf{pfn\_set(entryv nextpaddr)}}$
  $\specline{\ghostmaptoken{\delta{}s}{\rtv}{\delta} \ast \begin{array}{l} \ulcorner\textsf{entry\_present (pte\_initialize(pfn\_set(entry\_val nextpaddr)),level)}\urcorner \wand \\  \;\;\;\;\;\;\; \forall_{i\in\textsf{0..511}} \; \textsf{table\_root (pte\_initialized (pfn\_set (entry\_val nextpaddr))) + i * 8} \mapsto_{\textsf{id}} \textsf{level-1}    \end{array}  }_{\rtv}$
}
\end{lstlisting}
\vspace{-1em}
\caption{Allocating a physical page }
\label{pteinitializespecC}
\vspace{-1em}
\end{figure}

If the entry is not set, \textsf{pte\_initialize} (Line \ref{line:call_to_pte_initializeC} in Figure \ref{fig:calltopteinitializeC}) 
allocates a physical page (internally utilizing the only unverified (trusted) code in our case studies, the page-allocator's \textsf{kalloc},\footnote{
  This is an allocator for regions of pre-zeroed physical memory that is mapped, but not accessed by the allocator itself,
  as is typical for slab allocators~\cite{bonwick1994slab}.
  Its verification would be similar to verifying a usermode \textsf{malloc} verification ~\cite{Chlipala2013Bedrock,wickerson2010explicit},
  just with additional invariants on the memory pool.
} 
on Line \ref{line:call_to_kallocC} in Figure \ref{pteinitializespecC}). 
Since we are using \textsf{pte\_initialize} for page-table address allocation, we must relate this newly
allocated physical address to the identity mapping map $\Xi$ --- 
see Line \ref{line:page_of_capsC} in Figure \ref{fig:calltopteinitializeC}, where
\texttt{kalloc}'s specification guarantees it has returned memory from a designated memory
pool that is already mapped
\ifPLDI
\else
\footnote{A reasonable reader might wonder where this pool
initially comes from and how it might grow when needed. Typically an initial mapping subject to this identity mapping
constraint is set up prior to transition to 64-bit kernel code (notably,
a page table must exist \emph{before} virtual memory is enabled during boot, as part of enabling it is setting
a page table root).
Growing this pool later requires cooperation of physical memory range allocation and virtual memory range allocation,
typically by starting general virtual address allocation at the highest physical memory address plus the identity mapping offset.
This reserves the virtual addresses corresponding to all physical addresses plus the offset for later use in this pool,
as needed.
} 
\fi
and satisfies the offset invariants.
% \todo[inline,color=blue]{colin frontier.
% Stuck with line 31 onwards in Figure 7. rax holds nextpaddr, but I think that should be entrypfn, and 
% the explicit entrypfn id token assertion should go away, as its covered by the forall assertion.
% then the postcondition for pte-initialize should have a specific level now for the entries,
% like 0, which can be updated in the view shift on line 42.
% }
% Focusing on the specification of \textsf{pte\_initialize} separately in Figure \ref{fig:pteinitializespec}, 
% we right immediately realize that instead of seeing see a physical pointsto for the fresly page-table address 
% (e.g. $\mathsf{nextpaddr} \mapsto_{\mathsf{p}} \mathsf{w64\_0}$) deliberately in the post-conditoin in Lines 15-16,
%  we observe a full-ownership token representing the knowledge that a frame and all the entries indexed from this 
% frame are freshly allocated with full-ownership to be a part of the identity map, $\Xi$. 
The soundness argument of this specification relies on the fact that these freshly allocated resources are part
of an entry construction that has not been completed yet: the presence bit is set (line \ref{line:install_new_entryC} in Figure \ref{fig:calltopteinitializeC}) after these freshly allocated resources are incorporated into the
entry construction via the page frame portion of the PTE. In other words, the side condition, (*),
 formalizes that any access to the entry with these resources is \textit{invalid} (in the sense of not necessarily
having accompanying resources) until the entry is marked present (and thus the memory returned from \textsf{kalloc}
moves into the page table invariant).

\subsubsection{Physical-to-Virtual Conversion with \textsf{P2V}}
\label{sec:p2vC}
Once we know that the entry refers to a physical address in the identity mapping range ($\Xi$)
(via the branch at Line \ref{line:check_entry_present_jumpC}, or by allocating and installing a new entry
as just discussed for Lines \ref{line:check_entry_present_jumpC}--\ref{line:end_of_allocation_pathC}), 
we can convert this frame address to a corresponding virtual address via the identity mappings
discussed in Section \ref{subsec:identitymappingsC} and Figure \ref{fig:peraspaceinvariant_with_p2v_extensionC}.
in the last lines of \lstinline|pte_get_next_table| shown in Line \ref{line:p2vC} in Figure \ref{fig:calltopteinitializeC} (the continuation of Figure \ref{fig:calltopteinitializeC}).
This is a critical piece of the full page table walk verification.
In our small kernel (Line \ref{line:p2vC} in Figure \ref{fig:calltopteinitializeC}), as in larger kernels, the C macro \texttt{P2V} common to many kernels
is actually just addition by the constant offset mentioned in Section \ref{subsec:identitymappingsC}.
But the correctness of this simple instruction is quite subtle.
%  and cannot be proven 
% without the extended invariant (Figure \ref{fig:peraspaceinvariant_with_p2v_extension})
% worked out Section \ref{subsec:identitymappings}.

%\begin{figure}\footnotesize
%  \begin{lstlisting}[mathescape,escapeinside={(*}{*)}]
%  /*... Continued from Figure (*\ref{fig:calltopteinitialize}*); assertions below specialized to non-allocating path for clarity */
%  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus \{ \mathsf{entry} \}),m)  \ast \textsf{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{rcx}  \mapsto_{\textsf{r}} \textsf{\_}  \ast \textsf{entry} \mapsto_{\textsf{id}} \textsf{\_} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
%  $\specline{ \textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{(pte\_initialized (entry\_val.pfn))} \urcorner }_{\rtv}$
%  $\specline{ \texttt{rbp-16} \mapsto_{\textsf{v}} \textsf{(pte\_initialized (entry\_val.pfn)))} \ast \texttt{rax} \mapsto_{\textsf{r}} \textsf{ table\_root (pte\_initialize(entry\_val.pfn))} }_{\rtv}$
%  $\specline{\forall_{i\in \textsf{0 ... 511} } \ldotp  \ghostmaptoken{\textsf{id}}{((\textsf{table\_root (pte\_initialized (entry\_val.pfn)))}) + \textsf{i * 8})}{\textsf{v-1}}  }$ (*\label{line:children}*)
%  uintptr_t next_phys_addr = PTE_PFN_TO_ADDR(entry->pfn)
%  uintptr_t next_virt_addr = (uintptr_t) P2V(next_phys_addr); (*\label{line:p2v}*) 
%  /*... clean up the stack and return the rax value*/
%  return next
%}
%\end{lstlisting}
%\vspace{-1em}
%\caption{Converting a physical address of a PTE to a virtual address (w/o instruction pointer or flag updates).
% % Abreviating the other relevant resources (including the lower entries) peeled-out of the invariant as $\textsf{R}_{\textsf{children}}$
%\vspace{-1em}
%}
%\label{fig:p2v}
%\end{figure}
Lines \ref{line:childrenC}-\ref{line:p2vC} in Figure \ref{fig:calltopteinitializeC} show the verification of the end of \lstinline|pte_get_next_table| specialized to the case where no allocation was necessary (i.e., the conditional on Line \ref{line:check_entry_presentC} of Figure \ref{fig:calltopteinitializeC} was taken).
In this case, the true present bit allows access to the child tokens from Line \ref{line:conditional_childrenC} of Figure \ref{fig:calltopteinitializeC},
which is then refined to the assertion on Line \ref{line:childrenC} of Figure \ref{fig:calltopteinitializeC}.
The code loads the value at the offseted (\textsf{KERNBASE}) virtual memory address ($\textsf{entry}_{\textsf{pfn}}$ \textsf{+KERNBASE})
of the \emph{base} of the next level of the page table.
% \todo[inline]{the next sentence depends on having figure 10 updated to reflect the page-worth of tokens}
While we could now convert this address to a virtual points-to, this is not necessarily the correct thing to do.
The caller \lstinline|walkpgdir| (discussed next) uses \lstinline|pte_get_next_table| to retrieve just the base address,
because only the caller knows which entry in the subsequent table will be accessed (it depends on the corresponding bits from the virtual
address being translated). So instead we pass back the per-address-space invariant with the identity mapping resources for \lstinline|entry|
pulled out. The caller determines which entry in that table must actually
be accessed --- by selecting the appropriate index into the 512 ghost map tokens returned in the postcondition,
and using the ghost translation and physical location portions of the invariant to assemble a vpte-pointsto
that justifies the caller's subsequent access to a particular entry of the returned table.
% in the identity map ($\Xi\setminus\{entry\}$) of the kernel invariant.
% the logical update in Specification  Lines 5-10 to 10-14 for obtaining virtual-pointsto resource for the frame 
% ($\textsf{entry}_{\textsf{pfn}}$) by removing it from the ghost map ($\Xi\setminus\{entry\}\cup \{\textsf{entry}_{\textsf{pfn}}) \}$) 
% in Line 5 and compute the identity mapping for this physical frame address in Line 13 in Figure \ref{fig:p2v}).

\subsubsection{The Specification of \lstinline|pte_get_next_table|}
\hrule

Note that the specification does \emph{not} assume a specific page table level --- the logical parameter \textsf{v} represents the level
of the entry passed as an argument, and this code
is used for all three level transitions when traversing page tables (4 to 3, 3 to 2, 2 to 1).
This comes into play with a subtlety of the specification of \lstinline|pte_get_next_table| that we will
revisit several times: \lstinline|pte_get_next_table|'s specification
assumes it is given a virtual \emph{vpte-pointsto}
(a virtual points-to exposing the underlying physical address instead of existentially quantifying it;
 see Section \ref{sec:mapnewC}) granting access to the specified entry,
but its postcondition does not yield new virtual points-to assertions!
Instead it merely computes the base virtual address of the next table, and returns adequate capabilities (discussed in Section \ref{subsec:identitymappingsC})
for the \emph{caller} to construct a vpte-pointsto for any entry of the next table level (if this is not an L1 entry ---
the caller knows which level of the table this is for).
\looseness=-1


\subsubsection{Walking Page-Table Tree: Calling \textsf{pte\_get\_next\_table} for Each Level}
\label{wlkpgdirC}
\begin{figure}\footnotesize
\ifPLDI
\begin{lstlisting}[language=C,mathescape,escapeinside={(*}{*)}]
/* @param l4: the root address of the page-table tree
   @param va: virtual address to be translated and mapped */
pte_t *walkpgdir(pte_t *l4, const void *va) {
  $\specline{\textsf{P} \ast \ulcorner\textsf{rtv+KERNBASE}=\textsf{l4}\urcorner \ast \forall_{i\in\textsf{0..511}}\ghostmaptoken{\textsf{id}}{(\textsf{rtv}+i*8)}{\textsf{4}} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}}_{\rtv}$
  pte_t *l4_entry = &l4[L4Offset(va)]; // Virtual address of L4 entry(*\label{line:start_l4_calcC}*)
  /*At the 4th level (and other levels ignoring the level-specific indices) an access amounts to:*/
  /*1.Load l4 and va*/
  /*2.Extract 9-bit (0x1ff=511) L4 index from the virtual address va*/
  /*3.Multiply by 8 (byte index), add to l4 for entry address (*\label{line:end_l4_calcC}*) */
  /*4.Store virt. addr. to local var l4_entry; logical l4_entry is physical!*/
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry}\},m) \ast \forall_{i\in\textsf{0..511}\setminus\{\textsf{L4Offset(va)}\}}\ghostmaptoken{\textsf{id}}{(\textsf{rtv}+i*8)}{\textsf{4}}\ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
  $\specline{\textsf{entry\_present(l4e\_val)} \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(l4e\_val)+ i * 8}}{\textsf{3}} }_{\rtv} $
  $\specline{ \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l4\_entry l4e\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l4e\_val})\urcorner}_{\rtv}$    
  pte_t *l3 = pte_get_next_table(l4_entry); (*\label{line:first_getnext_callC}*) 
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry}\},m)  \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
  $\specline{\forall_{i\in\textsf{0..511}} \ghostmaptoken{\textsf{id}}{\textsf{table\_root(l4e\_val'.pfn) + i * 8}}{\textsf{3}}  \ast \ulcorner\textsf{l3-KERNBASE}=\textsf{table\_root(l4e\_val'.pfn)}\urcorner} $
  $\specline{ \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l4\_entry l4e\_val'} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l4e\_val})\urcorner}_{\rtv}$(*\label{line:first_pte_pointstoC}*)
  /*pte_get_next_table may have allocated a new page which updates entry*/
  pte_t *l3_entry = &l3[L3Offset(va)]; /* Virtual address of L3 entry*/
  $\specline{\ulcorner\textsf{l3}+\textsf{L3Offset(va)*8} = \textsf{l3\_entry} \land \textsf{table\_root(l4e\_val'.pfn)}  = \textsf{l3} \urcorner }_{\rtv}$
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry},\textsf{l3\_entry}\},m)  \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
  $\specline{\forall_{i\in\textsf{0..511}\setminus\{\textsf{L3Offset(va)}\}} \; \textsf{table\_root(l4\_entry}_{\textsf{pfn}}) \textsf{ + i * 8} \mapsto_{\textsf{id}} \textsf{3} }_{\rtv}$
  $\specline{ \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac4}}\textsf{l4\_entry}\; \textsf{l4e\_val'} \ast \ulcorner  \textsf{qfrac4} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l4e\_val'})\urcorner}_{\rtv}$(*\label{line:ex_l4_vpteC}*)
  $\specline{ \textsf{l3\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac3}}\textsf{l3\_entry}\; \textsf{l3e\_val} \ast \ulcorner  \textsf{qfrac3} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l3e\_val})\urcorner}_{\rtv}$(*\label{line:ex_l3_vpteC}*)
  $\specline{\textsf{entry\_present(l3e\_val)} \wand \forall_{i\in\textsf{0..511}} \;\ghostmaptoken{\textsf{id}}{\textsf{table\_root(l3\_entry.pfn) + i * 8}}{ \textsf{2}} } _{\rtv} $
  /*Similar steps & proofs for levels 3 and 2*/
  pte_t *l2 = pte_get_next_table(l3_entry);
  pte_t l2_entry = &l2[L2Offset(va)];
  pte_t *l1 = pte_get_next_table(l2_entry);
  /*Return the address of L1 entry with a similar index computation*/
  return &l1[L1Offset(va)]; 
}
$\specline{\textsf{R}_{\textsf{walk}} \ast \textsf{R}_{\textsf{l1e}} }_{\rtv}$
\end{lstlisting}

\else
\todo[inline]{this is dead code, we're using the PLDI flag set to true}
% \begin{lstlisting}[mathescape,escapeinside={(*}{*)}]
% ;;pte_t *walkpgdir(pte_t *l4, const void *va) {
% ... ;; Stack setup
% $\specline{\textsf{P} \ast \ulcorner\textsf{rtv+KERNBASE}=\textsf{l4}\urcorner \ast \forall_{i\in\textsf{0..511}}\ghostmaptoken{\textsf{id}}{(\textsf{rtv}+i*8)}{\textsf{4}} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}}_{\rtv}$
% ;;set up the stack for root address and virtual address    
% ;;pte_t *l4_entry = &l4[L4EX(va)]; // Virtual address of L4 entry(*\label{line:start_l4_calc}*)
% mov    -0x8[rbp],rsi
% mov    -0x10[rbp],rdi
% shr    0x27,rdi       ;; Shift L4 index to lowest bits
% and    0x1ff,rdi      ;; Mask to just lower 9 bits (0x1ff=511)
% shl    0x3,rdi        ;; Multiply by 8
% add    rdi,rsi        ;; Add to l4 (virtual) table base(*\label{line:end_l4_calc}*)
% mov    rsi,-0x18[rbp] ;; Store to local variable l4_entry; logical l4_entry is physical, program variable is virtual
% $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry}\},m) \ast \forall_{i\in\textsf{0..511}\setminus\{\textsf{L4EX(va)}\}}\ghostmaptoken{\textsf{id}}{(\textsf{rtv}+i*8)}{\textsf{4}}\ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
% $\specline{\textsf{entry\_present(l4e\_val)} \wand \forall_{i\in\textsf{0..511}} \; \ghostmaptoken{\textsf{id}}{\textsf{table\_root(l4e\_val)+ i * 8}}{\textsf{3}} }_{\rtv} $
% $\specline{ \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l4\_entry l4e\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l4e\_val})\urcorner}_{\rtv}$    
% ;;pte_t *pdp = pte_get_next_table(l4_entry);
% mov    -0x18[rbp],rdi
% ...
% callq  c0 <pte_get_next_table>(*\label{line:first_getnext_call}*)
% ;;save the physical next table address in rax
% $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry}\},m)  \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
% $\specline{\forall_{i\in\textsf{0..511}} \ghostmaptoken{\textsf{id}}{\textsf{table\_root(l4e\_val'.pfn) + i * 8}}{\textsf{3}}  \ast \ulcorner\textsf{pdp-KERNBASE}=\textsf{table\_root(l4e\_val'.pfn)}\urcorner} $
% $\specline{ \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l4\_entry l4e\_val'} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l4e\_val})\urcorner}_{\rtv}$(*\label{line:first_pte_pointsto}*)
% $\specline{\texttt{rax}  \mapsto_{\textsf{r}} \textsf{table\_root(l4e\_val'.pfn)} }_{\rtv}$ ;; pte_get_next_table may have allocated a new page, updating entry
% ;;pte_t *pdp_entry = &pdp[PDPEX(va)]; // Virtual address of L3 entry
% $\specline{\ulcorner\textsf{pdp}+\textsf{PDPEX(va)*8} = \textsf{pdp\_entry} \land \textsf{table\_root(l4e\_val'.pfn)}  = \textsf{pdp} \urcorner }_{\rtv}$
% $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry},\textsf{pdp\_entry}\},m)  \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$
% $\specline{\forall_{i\in\textsf{0..511}\setminus\{\textsf{PDPEX(va)}\}} \; \textsf{table\_root(l4\_entry}_{\textsf{pfn}}) \textsf{ + i * 8} \mapsto_{\textsf{id}} \textsf{3} }_{\rtv}$
% $\specline{ \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac4}}\textsf{l4\_entry}\; \textsf{l4e\_val'} \ast \ulcorner  \textsf{qfrac4} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l4e\_val'})\urcorner}_{\rtv}$(*\label{line:ex_l4_vpte}*)
% $\specline{ \textsf{pdp\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac3}}\textsf{pdp\_entry}\; \textsf{l3e\_val} \ast \ulcorner  \textsf{qfrac3} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l3e\_val})\urcorner}_{\rtv}$(*\label{line:ex_l3_vpte}*)
% $\specline{\textsf{entry\_present(l3e\_val)} \wand \forall_{i\in\textsf{0..511}} \;\ghostmaptoken{\textsf{id}}{\textsf{table\_root(pdp\_entry.pfn) + i * 8}}{ \textsf{2}} } _{\rtv} $
% ;;pte_t *pd = pte_get_next_table(pdp_entry);
% ... ;; Similar assembly to reach next level
% ;;pte_t *pd_entry = &pd[PDEX(va)]; // Virtual address of L2 entry
% ;;pte_t *pt = pte_get_next_table(pd_entry);
% $\specline{ \left( \begin{array}{l} \textsf{pdp}+\textsf{PDPEX(va)} = \textsf{pdp\_entry} \land \textsf{table\_root(l4\_entry}_{\textsf{pfn}})  = \textsf{pdp} \\ \textsf{pd}+\textsf{PDEX(va)} = \textsf{pd\_entry} \land \textsf{table\_root(pml3\_entry}_{\textsf{pfn}})  = \textsf{pd} \\ \textsf{pt}+\textsf{PDPEX(va)} = \textsf{pt\_entry} \land \textsf{table\_root(pml2\_entry}_{\textsf{pfn}})  = \textsf{pt} \\  \textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry},\textsf{pdp\_entry},\textsf{pd\_entry},\textsf{pt\_entry}\},m)  \\ \ghostmaptoken{\delta{}s}{\rtv}{\delta} \\
% \ghostmaptoken{\textsf{id}}{(\mathsf{l4\_entry})}{\textsf{4}}\ast \ghostmaptoken{\textsf{id}}{(\mathsf{plm3\_entry})}{\textsf{3}} \ast \ghostmaptoken{\textsf{id}}{(\mathsf{plm2\_entry})}{\textsf{2}}  \\ \forall_{i\in\textsf{0..511}} \; \textsf{table\_root(l4\_entry}_{\textsf{pfn}}) \textsf{ + i * 8} \mapsto_{\textsf{id}} \textsf{3}  \\ \forall_{i\in\textsf{0..511}} \; \textsf{table\_root(pdp\_entry}_{\textsf{pfn}}) \textsf{ + i * 8} \mapsto_{\textsf{id}} \textsf{2}  \\ \forall_{i\in\textsf{0..511}} \; \textsf{table\_root(pd\_entry}_{\textsf{pfn}}) \textsf{ + i * 8} \mapsto_{\textsf{id}} \textsf{1}  \\  \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l4e\_val} \\  \textsf{pdp\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l3e\_val} \\  \textsf{pd\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l2e\_val} \\   \textsf{pt\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l1e\_val} \ast \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l1e\_val})\urcorner    \end{array} \right)  =\textsf{R}_{\textsf{walk}}}$
% ;;access and return L1 entry
% ;;return &pt[PTEX(va)]; // Virtual address of L1 entry
% ...
% ;; clean up the stack 
% \end{lstlisting}
\fi
\todo[inline]{clarify virt/phys of input root and the output}
\caption{Walking page-table directory via calls to \textsf{pte\_get\_next\_table} in Figure \ref{fig:calltopteinitializeC}}
\label{walkpgdirC}
\vspace{-1em}
\end{figure}

\begin{figure}\footnotesize
$\begin{array}{l}\left( \begin{array}{l} \ulcorner \textsf{l3}+\textsf{L3Offset(va)} = \textsf{l3\_entry} \land \textsf{table\_root(l4\_entry}_{\textsf{pfn}})  = \textsf{l3} \\ \land \textsf{l2}+\textsf{L2Offset(va)} = \textsf{l2\_entry} \land \textsf{table\_root(l3\_entry}_{\textsf{pfn}})  = \textsf{l2} \\ \land \textsf{l1}+\textsf{L1Offset(va)} = \textsf{l1\_entry} \land \textsf{table\_root(l2\_entry}_{\textsf{pfn}})  = \textsf{l1} \urcorner  \ast \\  \textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi\setminus\{\textsf{l4\_entry},\textsf{l3\_entry},\textsf{l2\_entry},\textsf{l1\_entry}\},m)  \\ \ghostmaptoken{\delta{}s}{\rtv}{\delta} \ast  
      \ghostmaptoken{\textsf{id}}{(\mathsf{l4\_entry})}{\textsf{4}}\ast \ghostmaptoken{\textsf{id}}{(\mathsf{l3\_entry})}{\textsf{3}} \ast \ghostmaptoken{\textsf{id}}{(\mathsf{l2\_entry})}{\textsf{2}}  \ast \\  \textsf{l4\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l4\_entry} \; \textsf{l4e\_val}  \ast \\  \textsf{l3\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l3\_entry}\;\textsf{l3e\_val}  \ast \\  \textsf{l2\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l2\_entry} \; \textsf{l2e\_val}  \end{array} \right)  =\textsf{R}_{\textsf{walk} } 
    \\ \ast \left(\begin{array}{l}  \textsf{l1\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l1\_entry}\;\textsf{l1e\_val} \ast \\ \ulcorner  \textsf{qfrac} = 1 \leftrightarrow \; \lnot(\textsf{entry\_present l1e\_val})\urcorner \end{array}\right) =\textsf{R}_{\textsf{l1e}}\end{array} $
\todo[inline]{Colin: This is confusing: in the code \lstinline|l2_entry| is an actual entry word, but in these invariants they seem to be paddrs.}
\caption{\textsf{R}$_{\textsf{walk}}$: Resources obtained from invariant for walking page-table directory with \emph{non-existing entries} via calls to \textsf{l1e\_get\_next\_table} in Figure \ref{fig:calltopteinitializeC}}
\label{fig:rwalkC}
\vspace{-1em}
\end{figure}
Implementing a software page-table walk amounts to calling \textsf{pte\_get\_next\_table} for each level as shown in Figure \ref{walkpgdirC}. 
The key part of the specification and proof for a page table walk is accumulation of memory mappings for the page-table entries 
visited and frame addresses for page-tables. 
For example, Lines \ref{line:ex_l4_vpteC} and \ref{line:ex_l3_vpteC} in Figure \ref{walkpgdirC} show the virtual pte-pointsto assertions for L4 and L3 entries.
In the final post-condition, we expect the accumulation of these resources from each level -- $\textsf{R}_{\textsf{walk}}$ -- 
which allows us to construct and return the path to the L1 entry in the tree to insert a new page.  

This is the code which performs most actual physical-to-virtual conversions using the identity mapping portion of the per-address-space invariant.
\lstinline|walkpgdir| accepts a \emph{virtual} pointer to the base of the L4 table, and the address to translate.
The precondition provides knowledge that the virtual base of the L4 is at the appropriate offset from the current \lstinline|cr3| value,
but does not provide a virtual points-to assertion --- because the function must calculate (Lines \ref{line:start_l4_calcC}--\ref{line:end_l4_calcC})
which entry it needs access.
Instead, the precondition has 512 identity map tokens, guaranteeing that every entry on the page is subject to the identity mapping invariant.
Line \ref{line:end_l4_calcC} calculates the virtual address of the relevant entry, and the subsequent view shift
pulls that entry out of the identity mapping ($\Xi$) and fetches its corresponding resources as
described by Figure \ref{fig:peraspaceinvariant_with_p2v_extensionC} and Section \ref{subsec:identitymappingsC}.
The ghost translation and physical location are used to form the virtual pte-pointsto for the L4 entry
(Line \ref{line:first_pte_pointstoC}), with entry validity and next-level indexing
satisfying the rest of the precondition for \lstinline|pte_get_next_table|.
Then, as described earlier, checks the valid bit in the indicated
entry and either returns the (unconditional) tokens for the L3 entry physical addresses (if valid), or
allocates into the entry and returns new (also unconditional) tokens for the L3 entry physical addresses.
\lstinline|pte_get_next_table|'s first call (Line \ref{line:first_getnext_callC}) returns
the virtual address of the base of the L3 table. Then the situation to move from that pointer to the base of the L2
is just like the process just followed: the proof calculates the address of the relevant
L3 entry, uses the appropriate L3 identity mapping token to construct a virtual pte-pointsto to that entry,
and passes that along with additional resources pulled out of the invariant to another call to
\lstinline|pte_get_next_table|. That call then returns the base of an L2 table, and the process
repeats until the function returns the virtual address of the relevant L1 entry.
That will then be used in the next section by the caller of \lstinline|walkpgdir|
to install a new mapping.


% \textsf{walkpgdir}, as a client, holds the knowledge that there exists an identity mapping for the physical entry address (\textsf{entry})
%  in the root page table ($\textsf{L}_{4}$):  $\mathsf{entry} \mapsto_{\textsf{id}} \textsf{\_}$ in Specification Line 3 is a partially owned
%  token for accessing and looking up the resources in the identity map, $\Xi$, to construct the \textit{virtual-to-physical} pointsto relation 
% $\textsf{entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{entry \entry\_val}$ with the virtual address (\textsf{entry+KERNBASE}) obtained 
% by offsetting the physical address (\textsf{entry}). With this knowledge on the root-page-table-entry, we can start traversing the page-table 
% tree which requires locating the address of the next table -- a call to \textsf{pte\_get\_next\_table} shown in Figure \ref{fig:calltopteinitialize}. 
% Beyond a frame, the precondition before Line 15 requires the current address space invariant, and knowledge that \textsf{entry} is mapped to a 
% random entry value, subtly, 
% the operation also, at least, requires that the relevant table entry is readable, but the exact portion of ownership 
% returned must be determined by inspecting the valid bit
% of the value in memory --- so full ownership is returned only for unused entries.
% This is a simple piece of code whose functionality is critical and whose correctness is highly non-trivial. No prior work engages with this problem.



%\begin{figure}\footnotesize
%\begin{lstlisting}[mathescape]
%;;if (!entry->present) {
%  ...
%
%;;}
%...
%$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m)  \ast \texttt{rbp-8} \mapsto_{\textsf{v}} \textsf{entry} \ast \texttt{r8}  \mapsto_{\textsf{r}} \textsf{\_} \ast \texttt{rdi}  \mapsto_{\textsf{r}} \textsf{\_} \ast \sumpv\paddr\qfrac\true  \ghostmaptoken{\delta{}s}{\rtv}{\delta}  }_{\rtv}$ 
%mov    rax,-0x18[rbp]
 % 
%;;uintptr_t next_virt_addr = (uintptr_t) P2V(next_phys_addr);
%mov    -0x18[rbp],rax
%movabs KERN_BASE,rcx
%add    rcx,rax
%mov    rax,-0x20[rbp]
%\end{lstlisting}
%\vspace{-1em}
%\caption{Converting a physical address, whose exsitence is made certain, of a PTE to a virtual address (w/o instruction pointer or flag updates). $R$ abbreviates the resources
%associated with \paddr\ by $\Xi$ in Figure \ref{fig:peraspaceinvariant_with_p2v_extension}.}
%\label{fig:p2v}
%\end{figure}



%\caption{Traversing page-tables, and allocating entries as needed while mapping-a-page in Figure \ref{fig:mappingcode}.}
% \citet{kolanski08vstte,kolanski09tphols} verified a single code block with their logic which was roughly Figure \ref{fig:mapping_code} for a 2-level ARM
% page table, but several critical complexities our work deals with were not addressed.
% First, beyond the limitations discussed in Section \ref{sec:overly-restrictive}, Kolanski and Klein assumed that virtual addresses
% for page tables at each level were given as parameters rather than verifying any conversion from physical addresses to virtual addresses (or even axiomatizing their lookup).
% In contrast, our verification articulates the address space invariant from which the physical-to-virtual translation can be implemented.
% Second, our proof deals with the construction of a valid virtual points-to \emph{to the PTE to update} in mapping, which Kolanski and Klein also
% assumed was given.
% \todo{some of this is really an argument for our verification being more thorough, rather than being about our logic}

% Reasoning about the page table walk in their logic would have required 
% could reason about the walk, but would need to explicitly prove that all other invariants
% of the kernel, the current address space, and all other address spaces of interest were preserved by each update, because their model
% only supports separation within a single address space. In our model, this follows for free from making
% our separation logic directly aware of address translation and internalizing assumptions about other address spaces as further separable assertions.
% Kolanski and Klein did address part of the walk information for a 2-level page table (a possible ARM configuration), but 

% \textsc{seL4} currently still trusts address translations; it models page tables as a data structure in regular memory, thus not capturing the possibility that even
% temporarily destroying the mappings and restoring them can actually crash the OS. \textsc{CertiKOS} papers share little in the way of precise details about
% their virtual memory management, but because their core technology is based on a fork of \textsc{CompCert}, whose model of memory is
% a set of unordered block allocations, we can infer their proofs must also trust these translations.


\subsection{Mapping a New Page}
\label{sec:mapnewC}
One of the key tasks of a page fault handler in a general-purpose OS kernel is
to map new pages to an address space by writing into an existing page table via a call.\\
\centerline{\textsf{vaspace\_mappage(pte\_t *l4, void *va,uintptr\_t fpaddr)}}\\
in Figure \ref{fig:mapping_codeC}.
To do so, with a given allocated fresh page (\textsf{fpaddr}), then calculate the appropriate
known-valid page table walks (via \textsf{walkpgdir} Line \ref{line:call_walkpgdirC} in Figure \ref{fig:mapping_codeC}) and update
the appropriate L1 page table entry (Line 35 in Figure \ref{fig:mapping_codeC});
unmapping is the reverse of the logic we discuss here.
\looseness=-1
%\lstset{
%  columns=fullflexible,
%  numbers=left,
%  basicstyle=\ttfamily,
%  keywordstyle=\color{blue}\bfseries,
%  morekeywords={mov,add,call},
%  emph={rsp,rdx,rax,rbx,rbp,rsi,rdi,rcx,r8,r9,r10,r11,r12,r13,r14,r15},
%  emphstyle=\color{green},
%  emph={[2]cr3},
%  emphstyle={[2]\color{violet}},
%  morecomment=[l]{;;},
%  mathescape
%}
\begin{figure}\footnotesize
  \begin{lstlisting}[mathescape,escapeinside={(*}{*)}]
/*
@param plm4: the root address of the page-table tree
@param va: virtual address to be translated and mapped
@param fpaddr: physically allocated page's address in case the virtual address va is not backed with one
*/
void vaspace_mappage(pte_t *l4, void *va,uintptr_t fpaddr ) {
  $\specline{\textsf{P} \ast \textsf{l4} \mapsto_{\textsf{id}} \textsf{\_} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta} \ast \ulcorner\theta \; !!\; \vaddr = \texttt{None}\urcorner}_{\rtv}$      
  pte_t *pteaddr = walkpgdir(l4, va); (*\label{line:call_walkpgdirC}*)
  $\specline{ \textsf{R}_{\textsf{walk}} \ast \textsf{R}_{\textsf{l1e}} }$
  if (!pteaddr->present){ (*\label{line:mappage_pte_present_startC}*)
    /*The entry is not present(*\label{line:mappage_pte_present_endC}*)*/
    pteaddr->pfn = PTE_ADDR_TO_PFN(fpaddr); /*Store updated entry back to L1 entry (*\label{line:l1entry_storeC}*)*/
    $\specline{ \textsf{l1\_entry+KERNBASE} \mapsto_{\textsf{vpte,1}} \textsf{l1\_entry} \; (\textsf{set\_pfn}(\textsf{l1e\_val}, \textsf{PTE\_ADDR\_TO\_PFN}(\textsf{fpaddr}))) \ast  \ulcorner \lnot(\textsf{entry\_present l1e\_val}) \urcorner \ast \textsf{P}}$
    /*Set the present bit in entry*/
    pteaddr->present = 1;
  }
  return;
  $\specline{ \begin{array}{l}\textsf{l1\_entry+KERNBASE} \mapsto_{\textsf{vpte,qfrac}} \textsf{l1\_entry} \; \textsf{l1e\_val} \ast \ulcorner (\textsf{entry\_present l1e\_val}) \urcorner \ast \textsf{P} \\
 \lor \; \;  \textsf{l1\_entry+KERNBASE} \mapsto_{\textsf{vpte,1}} \textsf{l1\_entry} \; (\textsf{set\_pfn}(\textsf{l1e\_val}, \textsf{PTE\_ADDR\_TO\_PFN}(\textsf{fpaddr}))) \ast  \ulcorner \lnot(\textsf{entry\_present l1e\_val}) \urcorner \ast P \end{array} }$
}
\end{lstlisting}
\vspace{-1em}
  \caption{Specification of updating L1 entry to reference a new page (\textsf{fpaddr}).}
\label{fig:mapping_codeC}
\end{figure}

In Figure \ref{fig:mapping_codeC}, we see an address ($\vaddr$) currently not
mapped to a page ($\theta \; !!\; \vaddr = \texttt{None}$). Mapping a fresh
physical page to back the desired virtual page first requires ensuring
the existence of a memory location for an appropriate L1 table entry.
The code uses a helper function \lstinline{walkpgdir} (discussed again in Section \ref{wlkpgdirC}).
\textsf{walkpgdir}'s postcondition contains virtual \emph{PTE} points-to assertions ($\mapsto_{\textsf{vpte}}$)
both for ensuring partial page table walk reaching the
L1 entry (l1e) by asserting that higher levels of the page table exist (R$_{\textsf{walk}}$ in Figure \ref{fig:rwalkC}), 
and for allowing access to the memory of the L1 entry via virtual address (R$_{\textsf{l1e}}$ in Figure \ref{fig:rwalkC}).

A PTE points-to, as we already have briefly mentioned, is defined just like the normal virtual points-to of Figure \ref{fig:virtualpointstosharingC}, except the physical address (here, \textsf{pa}) is explicit in the assertion
rather than existentially quantified:\\
\centerline{$
    \vaddr\mapsto_{\textsf{vpte,q}} \; \paddr \; \vpage : \mathsf{vProp}~\Sigma \stackrel{\triangle}{=} 
    \exists \delta\ldotp
	(\lambda \mathit{cr3val}\ldotp
	\ghostmaptoken{\delta{}s}{\mathit{cr3val}}{\delta}) \ast 
  \fracghostmaptoken{\delta}{\vaddr}{\paddr}{\qfrac} \ast \paddr \mapsto_{\mathsf{p}} \vpage
$}\\
This supports rules for accessing memory
at that virtual address, but exposing the physical location being modified
makes this convenient to use for page table modifications, since we must ensure
the modified data is at the specific physical location that will be used as the L1 entry.
% After obtaining a virtual address \textsf{pte\_addr} in \textsf{rax} backed 
% by the physical memory for the L1 entry that will be used to translate the virtual addresses
% we are mapping, we save it to \textsf{r14} to be updated later in Line 9.

%In the precondition, we see Line 12 allocates a fresh page-aligned, zero-initialized page  (at \textsf{fpaddr}),
%returning a pre-filled PTE entry in \textsf{rax} ($+3$ sets the lower 2 bits).

% , to hold the freshly
% allocated physical page address (\textsf{fpaddr}) in Line X.

We already discussed for the upper level page-tables how the entry-present checks are handled.
However, for L1 entries this check is left to the caller of the
traversal function \textsf{walkpgdir}. In other words, unlike what we see in R$_{\textsf{walk}}$ for the upper levels, here all entry-present
checks have already been performed; the specification in R$_{\textsf{l1e}}$ ensures that page table entry for L1 needs to be checked at the caller site. 
In doing so, as we see in Figure \ref{fig:mapping_codeC}, the page reference \textsf{fpaddr} is linked back to the virtual address \textsf{va} 
only if it does not already refer to a physical resource (lines \ref{line:mappage_pte_present_startC}--\ref{line:mappage_pte_present_endC} in Figure \ref{fig:mapping_codeC}). 

The crucial step in addition to traversing the page table in Figure \ref{walkpgdirC} is actually updating the L1 entry (Line \ref{line:updatepfnC} in Figure \ref{fig:mapping_codeC}),
via the virtual address (\textsf{pt\_entry+KERNBASE}) known to translate to the appropriate physical address, in our example the L1
table entry address ($\textsf{PTE\_ADDR\_TO\_PFN(fpaddr)}$).

Unlike the only prior work verifying analogous code for mapping a new page~\cite{kolanski08vstte,kolanski09tphols}, our proof above
does \emph{not} need to reason directly over the operational semantics,
making this the first verification we know of for mapping a virtual memory page that
stays entirely at the program logic level.
\looseness=-1
% By incorporating verification of the
% \lstinline|ensure_L1| function (see Section \ref{sec:traversing}), our verification also directly handles several subtle aspects which
% were axiomatized in prior work.
\ifPLDI
\else
\subsection{Unmapping a Page}
\todo[inline]{update (esp. line refs) for new mapping code}
The reverse operation, unmapping a designated page that is currently mapped,
would essentially be the reverse of
the reasoning around line 22 above: given the virtual points-to assertions for all 512
machine words of memory that the L1 entry would map,
and information about the physical location, 
full permission on the L1 entry could be obtained, allowing the construction of a
full virtual PTE pointer for it, setting to 0, and reclaiming the now unmapped physical memory.
\fi


\begin{figure}\footnotesize
\begin{lstlisting}[mathescape,escapeinside={(*}{*)}]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,\Xi,m) \ast [\rtv'](\mathcal{I}\texttt{ASpace}(\theta',\Xi',m') \ast \texttt{Pother}) \ast \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \texttt{rbx}\mapsto_{\textsf{r}} \texttt{rbxv} }_{\rtv}$
$\specline{\texttt{rsp}\mapsto_{\textsf{r}} \texttt{rspv} \ast \texttt{rbp}\mapsto_{\textsf{r}} \texttt{rbpv} \ast \texttt{r12}\mapsto_{\textsf{r}} \texttt{r12v} \ast \texttt{r13}\mapsto_{\textsf{r}} \texttt{r13v} \ast \texttt{r14}\mapsto_{\textsf{r}} \texttt{r14v} \ast \texttt{r15}\mapsto_{\textsf{r}} \texttt{r15v}}_{\rtv}$
$\specline{\mathsf{ContextAt}(old,\_) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']) }_{\rtv}$
mov 0[rdi], rbx ... ;; also save rsp, rbp, r12, r13, r14, to offsets 8, 16, 24, 32, and 40 from rdi (*\label{line:start_saveC}*)
mov 48[rdi], r15
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,\Xi,m) \ast [\rtv'](\mathcal{I}\texttt{ASpace}(\theta',\Xi',m') \ast \texttt{Pother})  \ast  \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \texttt{rbx}\mapsto_{\textsf{r}} \texttt{rbxv} }_{\rtv}$
$\specline{ \texttt{rsp}\mapsto_{\textsf{r}} \texttt{rspv} \ast \texttt{rbp}\mapsto_{\textsf{r}} \texttt{rbpv} \ast \texttt{r12}\mapsto_{\textsf{r}} \texttt{r12v} \ast \texttt{r13}\mapsto_{\textsf{r}} \texttt{r13v} \ast \texttt{r14}\mapsto_{\textsf{r}} \texttt{r14v} \ast \texttt{r15}\mapsto_{\textsf{r}} \texttt{r15v}}_{\rtv}$
$\specline{\mathsf{ContextAt}(old,[\mathsf{rbxv,\ldots,\rtv}]) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']) }_{\rtv}$
mov 56[%rdi], %cr3 $\specline{\ldots \ast \texttt{rdi+56} \mapsto_{\textsf{v}} \rtv \ast \ldots }_{\rtv}$   (*\label{line:end_save}*) 
mov rbx, 0[rsi] (*\label{line:start_restoreC}*)
mov rsp, 8[rsi] ;; Switch to new stack, which may not be mapped in the current address space!
... ;; load rbp, r12, r13, r14, from offsets 16, 24, 32, and 40 from rsi
mov r15, 48[rsi]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,\Xi,m) \ast [\rtv'](\mathcal{I}\texttt{ASpace}(\theta',\Xi',m') \ast \texttt{Pother})  \ast  \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \texttt{rbx}\mapsto_{\textsf{r}} \texttt{rbxv}' }_{\rtv}$
$\specline{\texttt{rsp}\mapsto_{\textsf{r}} \texttt{rspv}' \ast \texttt{rbp}\mapsto_{\textsf{r}} \texttt{rbpv}' \ast \texttt{r12}\mapsto_{\textsf{r}} \texttt{r12v}' \ast \texttt{r13}\mapsto_{\textsf{r}} \texttt{r13v}' \ast \texttt{r14}\mapsto_{\textsf{r}} \texttt{r14v}' \ast \texttt{r15}\mapsto_{\textsf{r}} \texttt{r15v}'}_{\rtv}$
$\specline{\mathsf{ContextAt}(old,[\mathsf{rbxv,\ldots,\rtv}]) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']) }_{\rtv}$
mov cr3, 56[rsi] ;; <-- Switch to the new address space (*\label{line:end_restoreC}*)
$\specline{ [\rtv](\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,\Xi,m) \ast \mathsf{ContextAt}(old,[\mathsf{rbxv,\ldots,\rtv}]) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']))  }_{\rtv}$ (*\label{line:modality_switchC}*)
$\specline{ \mathcal{I}\texttt{ASpace}(\theta',\Xi',m') \ast \texttt{Pother} \ast  \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \ldots }_{\rtv'}$
\end{lstlisting}
\vspace{-1em}
\caption{Basic task switch code that switches address spaces.}
\label{fig:swtchC}
\end{figure}

\subsection{Change of Address Space}
A critical piece of \emph{trusted} code in current verified OS kernels is the assembly code to change the current address space; current verified OS kernels currently
lack effective ways to specify and reason about this low-level operation, for reasons outlined in Section \ref{sec:relworkC}.

Figure \ref{fig:swtchC} gives simplified code for a basic task switch, the heart of an OS scheduler implementation. This is code that saves the context (registers and stack)
of the running thread (here in a structure pointed to by \lstinline|rdi|'s value shown in Lines \ref{line:start_saveC}--\ref{line:end_saveC} of Figure \ref{fig:swtchC}) and restores the context of 
an existing thread (from \lstinline|rsi| shown in abbreviated Lines \ref{line:start_restoreC}--\ref{line:end_restoreC}), including the corresponding change of address space for a target thread in another process.
This code assumes the System V AMD64 ABI calling convention, where the normal registers not mentioned are caller-save, and therefore saved on the stack of the thread
that calls this code, as well as on the new stack of the thread that is restored; thus, only the callee-save registers and \texttt{cr3} must be
restored.\footnote{We are simplifying by ignoring non-integer registers (e.g., floating point, vector registers),
and the caller-save registers should be initialized to 0 to avoid leaking information across processes, but this captures the key challenges.}
With the addition of a return instruction, this code would satisfy the signatures of the C functions \footnote{This is the function in UNIX 6th Edition 
with the infamous ``You are not expected to understand this'' comment~\cite{lions1996lions}.}
\centerline{\lstinline[language=C]|void swtch(context_t* save, context_t* restore);|}\\
A call to this code begins by executing one thread (up to just before line \ref{line:end_saveC}) in an address space ($\rtv$), whose information will be saved in a structure at address $old$,
and finishes the execution by executing a different thread in a different address space (line \ref{line:end_restoreC} on), whose information is initially in $new$.

Because this code does not directly update the instruction pointer, it is worth explaining \emph{how} this switches threads: by switching address spaces and stacks. 
This is meant to be called with a return address for the current thread stored on the current stack when called. 
The precondition of the return address on the initial stack requires the callee-save register values at the time of the call: those stored in the first
half of the code.
Likewise, part of the invariant of the stack of the second thread, the one being restored, is that the return address on \emph{that} stack requires the saved callee-save registers stored in that context be in registers as its precondition.

The wrinkle and the importance of the modal treatment of assertions is that the target thread's precondition is \emph{relative to its address space}, 
not the address space of the calling thread, which is reflected by
the other-space modality
$[\rtv'](\mathcal{I}\texttt{ASpace}(\theta,\Xi,m) \ast \texttt{Pother})$
in the specfication. 
The precondition of this code,
in context, would include that the initial stack pointer (before \lstinline|rsp| is updated)
has a return address expecting the then-current callee-save register values and 
suitably updated (i.e., post-return) stack in the \emph{current} (initial) address space;
this would be part of \textsf{P} in the precondition.
The specification also requires that
the stack pointer saved in the context to restore expects the same of the saved registers and stack
\emph{in the other address space}. 
The other-space modality plays a critical role here; \textsf{Pother} would contain these assumptions in the other
address space.
\looseness=-1

% Lines 10--16 save the current context into memory (in the current address space).
% Line 22 saves the initial page table root.
% Lines 33--38 begin restoring the target context, including the stack pointer (line 33),
% which may not be mapped in the address space at that time: it is the stack for the context being
% loaded into the CPU.
% The actual address switch occurs on line 45, which is verified with our modal rule for updating \lstinline|cr3|,
% and thus shifts resources in and out of other-space modalities as appropriate.

The postcondition is analagous to the precondition, but interpreted \emph{in the new address space}: the then-current (updated) stack would have a return address expecting the new (restored) register values (again, in \textsf{Pother}),
and the saved context's invariant captures the precondition for restoring its execution \emph{in the previous address space} (as part of \textsf{P}). 

Immediately after the page table switch, assertions about the saved and restored contexts are
guarded by a modality for the retiring
address space \rtv{} (Line \ref{line:modality_switch}), per
\textsc{WriteToRegCtlFromRegModal} (Figure \ref{fig:wpdamd}),
because
there is no guarantee that the data structures of the previous address space are mapped in the new address space.
The ability to transfer that points-to information out of that modality is specific to a given kernel's design. 
Kernels that map kernel memory into all address spaces would need invariants
that justified moving those assertions out of the other-space modality.
% Following Spectre and Meltdown, this kernel design became less prevalent because speculative execution of accesses to kernel addresses could leak information even if the access did eventually cause a fault (the user/kernel mode permission check was done after fetching data from memory). Thus many modern kernels have reverted to the older kernel design where the kernel inhabits its own unique address space, and user processes have only enough extra material mapped in their address spaces to switch into the kernel (CPUs do not speculate past updates to \texttt{cr3}).
\looseness=-1

Although prior work has verified context switches within a single address space~\cite{ni2007contexts}, and context switches
without any code before or after~\cite{syeda2020formal} (that is, not reasoning about the impact of address space change
on what data were accessible), this is the first verification that handles both.
\looseness=-1

\begin{comment}
\[  
$\specline{\exists (\entryf ,\;\entrytr,\; \entrytw,\; \entryo,\;\textsf{pte\_addr },\paddr) \; \ldotp\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \_ \ast \texttt{rdi}\mapsto_{r} \vaddr \ast \texttt{rax}\mapsto_{\textsf{r}} \textsf{ pte\_addr} \; \ast }_{\rtv}$
$\specline{ \ulcorner  \texttt{addr\_L1 }(\vaddr, \entryo) = \paddr \urcorner \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner \; \ast}_{\rtv}$
$\specline{\nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \; \ast \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\entryo \;\ast \texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{pte\_addr}  }_{\rtv}$
mov r14, rax ;; Save that before another call
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \ast \texttt{rax}\mapsto_{\textsf{r}} \textsf{ pte\_addr} \; \ast }_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\entryo \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{pte\_addr}  }_{\rtv}$
call alloc_phys_page_or_panic
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \;\ast \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\naddr \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr\; (\texttt{wzero 64})  \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner}_{\rtv}$
$\specline{\exists \texttt{ fpaddr} \ldotp \ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{fpaddr+3} \ast \texttt{fpaddr} \mapsto_{\textsf{p}} (\texttt{wzero 64}) \ast \ulcorner \texttt{entry\_present (fpaddr+3)}\urcorner}_{\rtv}$
;; Calculate new L1 entry
mov [r14], rax ;; store the page table entry, mapping the page
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \;\ast \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\entryo \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{fpaddr+3}) \; \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner }_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{fpaddr+3} \ast \texttt{fpaddr} \mapsto_{\textsf{p}} (\texttt{wzero 64}) \ast \ulcorner \texttt{entry\_present fpaddr+3}\urcorner}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \ast }_{\rtv}$
$\specline{\textsf{L}_{4}\_\textsf{L}_{1}\_\textsf{PointsTo}(\vaddr,\entryf,\entrytr,\entrytw,\fpaddr+3) \ast \ulcorner \theta \;!!\;\vaddr = \texttt{None}\urcorner \; \ast}_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{fpaddr+3} \ast \texttt{fpaddr} \mapsto_{\textsf{p}} (\texttt{wzero 64}) }_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast}_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{fpaddr} \mapsto_{\textsf{p}} \textsf{ wzero 64} \ast \ghostmaptoken{\delta{}s}{\rtv}{\delta}  \ast\sumwalkabs\vaddr\qfrac\fpaddr}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast \vaddr \mapsto_{\textsf{vpte}}\; \{\qfrac\} \;\fpaddr \textsf{ wzero 64}}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast \vaddr \mapsto_{\textsf{v}}\; \{\qfrac\} \textsf{wzero 64}}_{\rtv}$
\end{comment}
