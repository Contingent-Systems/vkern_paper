%$\assert{\ulcorner \mathsf{aligned maddr} \urcorner \ast \mathsf{r14} \mapsto_{\textsf{r}} \textsf{r14v} \ast \mathsf{r13} \mapsto_{\textsf{r}} \textsf{maddr} \ast \mathsf{rdi} \mapsto_{\textsf{r}} \textsf{rdiv} \ast \mathsf{rax} \mapsto_{\textsf{r}} \textsf{raxv}}$
%$\assert{\ulcorner \ptablestore !! \textsf{maddr} = \textsf{None} \urcorner \ast \ownGhost\gammaPred{\authfull{\ptableabswalk\ptablestore}} \ast \textsf{Pf}} $
\section{Experiment}
\label{sec:experiment}
To both validate and demonstrate the value of the modal approach to reasoning about virtual memory management, we study several distillations of real concerns of virtual memory managers.
Recall from Section \ref{sec:logic} that virtual points-to assertions work just like regular points-to assertions, by design.

%\begin{comment}
%\todo[inline]{Identity mappings are difficult, and our current approach won't quite work. Consider trying to have a virtual pointsto for an actual page table entry (i.e., that one could use to update a page table mapping), while also having a virtual pointsto for an address that entry mapped. With the current (let's call it v1) solution, we can't actually have both of those simultaneously!  That's because the PTE pointsto will assert full ownership of the physical memory cell holding the PTE as its data value, while the virtual pointsto for the data mapped by that entry will \emph{also} assert (fractional) ownership of all entries a page table walk would traverse.
%}
%\todo[inline,color=violet]{This doesn't seem to cause issues with the mapping/unmapping examples, only with changing intermediate page table pointers. The mapping example requires a virtual pointsto for the blank PTE, and once filled in that ownership can be immediately split to create the 512 new virtual pointsto assertions for the newly mapped page. Conversely, for unmapping we'd assume ownership of all the relevant virtual pointsto assertions for the page we're unmapping, at which point we can (with a bit of work) show that they all correspond to the same L1 PTE, and extract the 512 fractional shares of that entry from the pointsto assertions.  But changing intermediate page tables, as one would do for coallescing or splitting a superpage while preserving the virtual-to-physical mappings, couldn't be done without some really complicated separating implication tricks.}
%\todo[inline,color=green]{One possible approach to resolving this, which we came up with in our Tuesday meeting, is to recognize that the current (v1) virtual points-to is too strong, because it really doesn't care about \emph{owning} those fractional resources, it only cares that \emph{something} ensures the correct page table walk exists. Iris has a ghost map resource where authoritative ownership of an individual key-value pair can be handled as a resource.  (Colin was using this in the filesystem cache.)
%We can use that mechanism to separate the virtual-to-physical translation from the physical memory involved (Kolanski and Klein may have done something similar for different reasons): (fractional) virtual points-to assertions can be defined in terms of (fractional) ownership of these authoritative ghost map entry assertions, plus sharing an invariant that the current installed page table respects all entries of the mapping. Unmapping collects the authoritative map kvpairs from collecting the assertions, and then can remove them from the ghost map and update the page tables. Critically, physical ownership of the page tables then lives in the invariant on the current page table, so some virtual pointsto assertions can refer to memory in those page tables.
%This still works with the modality, since that invariant is also semantically a predicate on a page table root.
%Let's call this v2.
%}
%\end{comment}
\subsection{Mapping a New Page}
One of the key tasks of a page fault handler in a general-purpose OS kernel is to map new pages into an address space by writing into an existing page table. To do so, we first allocate a fresh page, then calculate the appropriate known-valid page table walks and update the appropriate L1 page table entry.
\lstset{
  columns=fullflexible,
  numbers=left,
  basicstyle=\ttfamily,
  keywordstyle=\color{blue}\bfseries,
  morekeywords={mov,add,call},
  emph={rsp,rdx,rax,rbx,rbp,rsi,rdi,rcx,r8,r9,r10,r11,r12,r13,r14,r15},
  emphstyle=\color{green},
  emph={[2]cr3},
  emphstyle={[2]\color{violet}},
  morecomment=[l]{;;},
  mathescape
}
\newcommand{\fpaddr}{\texttt{fpaddr}}
\newcommand{\specline}[1]{{\color{blue}\left\{#1\right\}}}
\begin{figure}\footnotesize
  \begin{lstlisting}
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m)  \ast  \texttt{r14}\mapsto_{r} \_ \ast \texttt{rdi}\mapsto_{r} \vaddr \ast \texttt{rax}\mapsto_{r} \_ \ast \ulcorner \texttt{aligned va} \land  \theta \; !!\; \vaddr = \texttt{None}\urcorner}_{\rtv}$
call ensure_L1_page
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{r} \_ \ast \texttt{rdi}\mapsto_{r} \vaddr \ast \texttt{rax}\mapsto_{r} \textsf{ pte\_addr} \; \ast }_{\rtv}$
$\specline{\exists (\entryf ,\;\entrytr,\; \entrytw,\; \entryo,\;\textsf{pte\_addr },\paddr \; : \texttt{word 64}) \ldotp \ulcorner \texttt{addr\_L1 }(\vaddr, \entryo) = \paddr \urcorner \; \ast }_{\rtv}$
$\specline{\nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \; \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\naddr \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \ast \texttt{rax}\mapsto_{r} \texttt{pte\_addr}  }_{\rtv}$
;; Returns the virtual address of the L1 entry in rax
mov %r14, %rax ;; Save that before another call
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{r} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{r} \vaddr \ast \texttt{rax}\mapsto_{r} \textsf{ pte\_addr} \; \ast }_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \; \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\naddr \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \ast \texttt{rax}\mapsto_{r} \texttt{pte\_addr}  }_{\rtv}$
call alloc_phys_page_or_panic
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{r} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{r} \vaddr \;\ast}_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\naddr \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \; \ast }_{\rtv}$
$\specline{\exists \texttt{ fpaddr} \ldotp \ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{r} \texttt{fpaddr} \ast \texttt{fpaddr} \mapsto_{a} (\texttt{wzero 64})}_{\rtv}$
;; Calculate new L1 entry
;; update the page table entry, mapping the page
mov (%r14), %rax
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{r} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{r} \vaddr \;\ast}_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\naddr \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{fpaddr}) \; \ast }_{\rtv}$
$\specline{\exists \texttt{ fpaddr} \ldotp \ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{r} \texttt{fpaddr} \ast \texttt{fpaddr} \mapsto_{a} (\texttt{wzero 64})}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{r} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{r} \vaddr \ast }_{\rtv}$
$\specline{\textsf{L}_{4}\_\textsf{L}_{1}\_\textsf{PointsTo}(\vaddr,\entryf,\entrytr,\entrytw,\paddr) \ast \ulcorner \theta \;!!\;\vaddr = \texttt{None}\urcorner \; \ast}_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{r} \texttt{fpaddr} \ast \texttt{fpaddr} \mapsto_{a} (\texttt{wzero 64}) }_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast}_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{fpaddr} \mapsto_{\textsf{p}} \textsf{ wzero 64} \ast \sumapaces\rtv\delta  \ast\sumwalkabs\vaddr\qfrac\fpaddr}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast \vaddr \mapsto_{\textsf{vpte}}\; \qfrac \;\fpaddr \textsf{ wzero 64}}_{\rtv}$
\end{lstlisting}
  \caption{Specification and proof of distilled code for mapping a new page with $\textsf{wpd\_def }_{\rtv}$ with a fixed namespace $\gammaPreds$ and a global map of address-spaces $m$  }
  \todo[inline,color=yellow]{Colin, could you speak about the fact that we have wpd in iProp so that's the reason to have rtval whereever table traversal is needed}
\label{fig:mapping_code}
\todo[inline]{this is terribly ugly, should switch to minted}
\end{figure}
In Figure \ref{fig:mapping_code}, we see an address ($\vaddr$) currently not mapped to a page ($\theta \; !!\; \vaddr = \texttt{None}$). Mapping a fresh page backing this virtual first requires an assurance on the existence L1 table entry which is realized in two folds:
\begin{itemize}
\item physical pointsto assertions on the tables L4, L3, and L2 reaching to the L1 level entry (l1e) ensures the successful table lookups (Specification Lines 4-6)
\item ,and a virtual L1 pointsto ($\mapsto_{\textsf{vpte}}$) on the address computed from the reached L1 entry ($ \ulcorner \texttt{addr\_L1}(\vaddr,\entryo) = \paddr\urcorner$) which is almost exactly same with the virtual-pointsto definition in Figure \ref{fig:virtualpointstosharing} except the physical page address is already computed, not exsitentially quantified:
 \[
\begin{array}{l}
    \vaddr\mapsto_{\textsf{vpte}} \;\{\textsf{q}\} \; \paddr \; \vpage : \mathsf{vProp}~\Sigma \stackrel{\triangle}{=} 
    \lambda \mathit{cr3val}\ldotp
    \exists \delta\ldotp
    \sumapaces{\mathit{cr3val}}\delta \ast 
  \sumwalkabs\vaddr\qfrac\paddr \ast \paddr \mapsto_{\mathsf{p}} \vpage
\end{array}
\]
\end{itemize}
After obtaining an virtual address \textsf{pte\_addr} in \textsf{rax} referring to a physical address, computed from already known L4-L2 physical memory pointstos ($\texttt{addr\_L1}(\vaddr,\entryo) = \paddr$),
in Specification Line 7 ($\texttt{pte\_addr} \mapsto_{\textsf{vpte}}  \; \paddr\;(\texttt{wzero 64})$), we save it to \textsf{r14} to be updated later in Line 9.

Then we allocate a fresh page initialized (i.e. zeroed) in Line 14 which return an \textsf{aligned} address of fresh page (\textsf{fpaddr}) in \textsf{rax} (Specification Line 19).

The crux point in the proof is updating the virtual L1 address (\textsf{pte\_addr}) to show the fresly allocated page address (\textsf{fpaddr}) in Line 22 to obtain
\[\texttt{pte\_addr} \mapsto_{\texttt{vpte}}  \; \paddr\; \textsf{fpaddr}  \qquad\qquad (\textsf{by Rule} \TirNameStyle{WriteToVirtMemFromReg})\]
by Rule \TirNameStyle{WriteToVirtMemFromReg} which allows proving virtual address updates without exposing its internal physical page-table memory accesses (Specification Line 26). 
Consequently, right at this point in the proof, when we unfold the $\texttt{pte\_addr} \mapsto_{\texttt{vpte}}  \; \paddr\; \textsf{fpaddr})$ we will be able obtain a full ownerhsip on the L1 table physical pointsto $\paddr \mapsto_{\mathsf{p}} \textsf{fpaddr}$ . Then, we can split this full-ownership into to obtain a fractional ownership (q4) on it ($\paddr \mapsto_{\mathsf{p}} \{q4\} \;\textsf{fpaddr}$). Finally, we can connect this fragmental ownership with the L4-L2 physical table pointstos (Specification Lines 24 and 25) to obtain the complete physical table-walk assertion (L4\_L1\_PointsTo(maddr l4e l3e l2e pa)) in Line 30.

Finally, we can insert $\vaddr$ into the ghost page-table-walk summarization map ($\theta$) to change its state from unmapped ($\ulcorner \theta \;!!\;\vaddr = \texttt{None}\urcorner$ in Specification Line 30) to mapped (Specification Line 33) by Iris' ghost-map update ($\sqsubseteq$), and construct virtual-pointsto $\vaddr \mapsto_{\textsf{vpte}}\; \qfrac \;\fpaddr \textsf{ wzero 64}$ (Specification Line 36) with $\sumwalkabs\vaddr\qfrac\fpaddr$ obtained from ghost page-table-walk insertions, $\sumapaces\rtv\delta$ from unfolding the definition of $\mapsto_{\textsf{vpte}}$ in Specification Line 18, and $\fpaddr \mapsto_{\textsf{p}} \textsf{wzero 64}$ in Specification Line 34. fragmental ownership of 
\subsection{Unmapping a Page}
The reverse operation, unmapping a designated page that is currently mapped, is pleasingly the reverse: by flipping the entry back to an invalid entry, full ownership of the updated entry reverts back to the now-invalid cell of the L1 table.

\subsection{Change of Address Space}
A critical piece of \emph{trusted} code in verified OS kernels is the assembly code to change the current address space; current verified OS kernels currently lack effective ways to specify and reason about this low-level operation, for resaons outlined in Section \ref{sec:relwork}.
\todo[inline]{probably not much more than just the cr3 update, but could be extended to a full context switch since the instruction pointer isn't updated in a swtch}
\begin{figure}\footnotesize
\begin{lstlisting}
;; Assume the save-space is in rdi,
;; load-space in rsi
;; No need to save/restore caller-save regs
;; Save yielding context
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'+56](\mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother})}_{\rtv}$
$\specline{ \texttt{rsi}\mapsto_{r} \rtv' \ast \texttt{rdi}\mapsto_{r} \_ \ast \texttt{rbx}\mapsto_{r} \texttt{rbxv} \ast  \texttt{rsp}\mapsto_{r} \texttt{rspv} \ast \texttt{rbp}\mapsto_{r} \texttt{rbpv} }_{\rtv}$
$\specline{\texttt{r12}\mapsto_{r} \texttt{r12v} \ast \texttt{r13}\mapsto_{r} \texttt{r13v} \ast \texttt{r14}\mapsto_{r} \texttt{r14v} \ast \texttt{r15}\mapsto_{r} \texttt{r15v}}_{\rtv}$
$\specline{\texttt{rdi+0} \mapsto_{v} \_ \ast \texttt{rdi+8} \mapsto_{v} \_ \ast \texttt{rdi+16} \mapsto_{v} \_ \ast \texttt{rdi+24} \mapsto_{v} \_ \ast \texttt{rdi+32} \mapsto_{v} \_}_{\rtv}$
$\specline{\texttt{rdi+40} \mapsto_{v} \_ \ast \texttt{rdi+48} \mapsto_{v} \_\ast \texttt{rdi+56} \mapsto_{v} \_}_{\rtv}$
mov 0[%rdi], %rbx
mov 8[%rdi], %rsp
mov 16[%rdi], %rbp
mov 24[%rdi], %r12
mov 32[%rdi], %r13
mov 40[%rdi], %r14
mov 48[%rdi], %r15
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'+56](\mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother})}_{\rtv}$
$\specline{ \texttt{rdi}\mapsto_{r} \_ \ast \texttt{rbx}\mapsto_{r} \texttt{rbxv} \ast  \texttt{rsp}\mapsto_{r} \texttt{rspv} \ast \texttt{rbp}\mapsto_{r} \texttt{rbpv} }_{\rtv}$
$\specline{\texttt{r12}\mapsto_{r} \texttt{r12v} \ast \texttt{r13}\mapsto_{r} \texttt{r13v} \ast \texttt{r14}\mapsto_{r} \texttt{r14v} \ast \texttt{r15}\mapsto_{r} \texttt{r15v}}_{\rtv}$
$\specline{\texttt{rdi+0} \mapsto_{v} \texttt{rbxv} \ast \texttt{rdi+8} \mapsto_{v} \texttt{rspv} \ast \texttt{rdi+16} \mapsto_{v} \texttt{rbpv} \ast \texttt{rdi+24} \mapsto_{v} \texttt{r12v} \ast}_{\rtv}$
$\specline{ \texttt{rdi+32} \mapsto_{v} \texttt{r13v} \ast \texttt{rdi+40} \mapsto_{v} \texttt{r14v} \ast \texttt{rdi+48} \mapsto_{v} \texttt{r15v}\ast \texttt{rdi+56} \mapsto_{v} \_}_{\rtv}$
mov 56[%rdi], %cr3
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'+56](\mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother})}_{\rtv}$
$\specline{ \texttt{rdi}\mapsto_{r} \_ \ast \texttt{rbx}\mapsto_{r} \texttt{rbxv} \ast  \texttt{rsp}\mapsto_{r} \texttt{rspv} \ast \texttt{rbp}\mapsto_{r} \texttt{rbpv} }_{\rtv}$
$\specline{\texttt{r12}\mapsto_{r} \texttt{r12v} \ast \texttt{r13}\mapsto_{r} \texttt{r13v} \ast \texttt{r14}\mapsto_{r} \texttt{r14v} \ast \texttt{r15}\mapsto_{r} \texttt{r15v}}_{\rtv}$
$\specline{\texttt{rdi+0} \mapsto_{v} \texttt{rbxv} \ast \texttt{rdi+8} \mapsto_{v} \texttt{rspv} \ast \texttt{rdi+16} \mapsto_{v} \texttt{rbpv} \ast \texttt{rdi+24} \mapsto_{v} \texttt{r12v} \ast }_{\rtv}$
$\specline{\texttt{rdi+32} \mapsto_{v} \texttt{r13v} \ast \texttt{rdi+40} \mapsto_{v} \texttt{r14v} \ast \texttt{rdi+48} \mapsto_{v} \texttt{r15v}\ast \texttt{rdi+56} \mapsto_{v} \rtv}_{\rtv}$    
;; Restore target context
mov %rbx, 0[%rsi] 
;; This switches to a new stack
;; This stack may not be mapped in the
;; current address space!
mov %rsp, 8[%rsi] 
mov %rbp, 16[%rsi]
mov %r12, 24[%rsi]
mov %r13, 32[%rsi]
mov %r14, 40[%rsi]
mov %r15, 48[%rsi]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'+56](\mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother})}_{\rtv}$
$\specline{ \texttt{rdi}\mapsto_{r} \_ \ast \texttt{rbx}\mapsto_{r} \rtv'+0 \ast  \texttt{rsp}\mapsto_{r} \rtv'+8 \ast \texttt{rbp}\mapsto_{r} \rtv'+16 }_{\rtv}$
$\specline{\texttt{r12}\mapsto_{r} \rtv'+24 \ast \texttt{r13}\mapsto_{r} \rtv'+32 \ast \texttt{r14}\mapsto_{r} \rtv'+40 \ast \texttt{r15}\mapsto_{r} \rtv'+48}_{\rtv}$
$\specline{\texttt{rdi+0} \mapsto_{v} \texttt{rbxv} \ast \texttt{rdi+8} \mapsto_{v} \texttt{rspv} \ast \texttt{rdi+16} \mapsto_{v} \texttt{rbpv} \ast \texttt{rdi+24} \mapsto_{v} \texttt{r12v} \ast }_{\rtv}$
$\specline{\texttt{rdi+32} \mapsto_{v} \texttt{r13v} \ast \texttt{rdi+40} \mapsto_{v} \texttt{r14v} \ast \texttt{rdi+48} \mapsto_{v} \texttt{r15v}\ast \texttt{rdi+56} \mapsto_{v} \rtv}_{\rtv}$
;; Switch to the new address space
mov %cr3, 56[%rsi]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'+56](\mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother}) }_{\rtv}$
$\specline{ \texttt{rdi}\mapsto_{r} \_ \ast \texttt{rbx}\mapsto_{r} \rtv'+0 \ast  \texttt{rsp}\mapsto_{r} \rtv'+8 \ast \texttt{rbp}\mapsto_{r} \rtv'+16 }_{\rtv}$
$\specline{\texttt{r12}\mapsto_{r} \rtv'+24 \ast \texttt{r13}\mapsto_{r} \rtv'+32 \ast \texttt{r14}\mapsto_{r} \rtv'+40 \ast \texttt{r15}\mapsto_{r} \rtv'+48}_{\rtv}$
$\specline{\texttt{rdi+0} \mapsto_{v} \texttt{rbxv} \ast \texttt{rdi+8} \mapsto_{v} \texttt{rspv} \ast \texttt{rdi+16} \mapsto_{v} \texttt{rbpv} \ast  \texttt{rdi+24} \mapsto_{v} \texttt{r12v} \ast }_{\rtv}$
$\specline{\texttt{rdi+32} \mapsto_{v} \texttt{r13v} \ast \texttt{rdi+40} \mapsto_{v} \texttt{r14v} \ast \texttt{rdi+48} \mapsto_{v} \texttt{r15v}\ast \texttt{rdi+56} \mapsto_{v} \rtv}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{ [\rtv](\texttt{Pcurrent}) \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother}}_{\rtv'+56}$
\end{lstlisting}
\caption{Basic task switch code that switches address spaces.}
\label{fig:swtch}
\end{figure}

Figure \ref{fig:swtch} gives simplified code for a basic task switch, the heart of an OS scheduler implementation. This is code that saves the context (registers and stack) of the running thread (here in a structure pointed to by \lstinline|rdi|'s value) and restores the context of an existing thread (from \lstinline|rsi|), including the corresponding change of address space for a target thread in another process.
This code assumes the System V AMD64 ABI calling convention, where the normal registers not mentioned are caller-save, and therefore saved on the stack of the thread that calls this code, as well as on the new stack of the thread that is restored, thus only the callee-save registers and \texttt{cr3} must be restored.\footnote{We are simplifying in a couple basic ways. First, we are ignoring non-integer registers (e.g., floating point, vector registers) entirely. Second, we are ignoring that the caller-save registers should still be initialized to 0 to avoid leaking information across processes. We focus on the core logical requirements.}
With the addition of a return instruction, this code would satisfy the C function signature\footnote{The name comes from the UNIX 6th Edition \lstinline|swtch| function, the source of the infamous ``You are not expected to understand this'' comment~\cite{lions1996lions}.}
\begin{lstlisting}[language=C]
void swtch(context_t* save, context_t* restore);
\end{lstlisting}
A call to this code begins executing one thread in one address space (whose information will be saved in \lstinline[language=C]|save|) and finishes execution executing a different thread in a different address space (whose information is initially in \lstinline[language=C]|restore|).

Because this code does not directly update the instruction pointer, it is worth explaining \emph{how} this switches threads: by switching stacks. This is meant to be called with a return address for the current thread stored on the current stack when called --- which must be reflected in the calling convention. In particular, the precondition of the return address on the initial stack requires the callee-save register values at the time of the call: those stored in the first half of the code.
Likewise, part of the invariant of the stack of the second thread, the one being restored, is that the return address on \emph{that} stack requires the saved callee-save registers stored in that context to be in registers as its precondition.

The wrinkle, and the importance of the modal treatment of assertions, is that the target thread's precondition is \emph{relative to its address space}, not the address space of the calling thread.
Thus the precondition of this code is that the then-current stack pointer has a return address expecting the then-current callee-save register values and suitablly updated (i.e., post-return) stack in the \emph{current} (initial) address space, while the stack pointer saved in the context to restore expects the same of the saved registers and stack \emph{in the other address space}. And the postcondition is analagous, but interpreted \emph{in a different address space}: the then-current (updated) stack has a return address expecting the new (restored) register values, and the saved context's invariant captures the precondition for restoring its execution \emph{in the previous address space}. At that point, it is safe to execute a return (or in a variant on this for user-mode and/or preemptive scheduling, a return-from-interrupt instruction).

In addition, immediately after the page table switch, the points-to information about the saved and restored contexts is \emph{also} guarded by a modality for the old address space, since there is no automatic guarantee that that memory is mapped in the new address space.  The ability to transfer that points-to information out of that modality is specific to a given kernel's design. Kernels that map kernel memory into all address spaces would need to ensure and specify enough specific details about memory mappings to allow a proof of an elimination rule for specific modally-constrained points-to assertions.
Following Spectre and Meltdown, this kernel design became less prevalent because speculative execution of accesses to kernel addresses could leak information even if the access did eventually cause a fault (the user/kernel mode permission check was done after fetching data from memory). Thus many modern kernels have reverted to the older kernel design where the kernel inhabits its own unique address space, and user processes have only enough extra material mapped in their address spaces to switch into the kernel (CPUs do not speculate past updates to \texttt{cr3}).


\todo[inline]{technically we weren't planning to prove reads of cr3...}
\todo[inline]{need to figure out the right pre/post condition to highlight the change in address space, beyond the vpoints-to of the two structures}
