%$\assert{\ulcorner \mathsf{aligned maddr} \urcorner \ast \mathsf{r14} \mapsto_{\textsf{r}} \textsf{r14v} \ast \mathsf{r13} \mapsto_{\textsf{r}} \textsf{maddr} \ast \mathsf{rdi} \mapsto_{\textsf{r}} \textsf{rdiv} \ast \mathsf{rax} \mapsto_{\textsf{r}} \textsf{raxv}}$
%$\assert{\ulcorner \ptablestore !! \textsf{maddr} = \textsf{None} \urcorner \ast \ownGhost\gammaPred{\authfull{\ptableabswalk\ptablestore}} \ast \textsf{Pf}} $
\section{Experiments}
\label{sec:experiment}
To both validate and demonstrate the value of the modal approach to reasoning about virtual memory management, we study several
distillations of real concerns of virtual memory managers.
Recall from Section \ref{sec:logic} that virtual points-to assertions work just like regular points-to assertions, by design.

%\begin{comment}
%\todo[inline]{Identity mappings are difficult, and our current approach won't quite work. Consider trying to have a virtual pointsto for an actual page table entry (i.e., that one could use to update a page table mapping), while also having a virtual pointsto for an address that entry mapped. With the current (let's call it v1) solution, we can't actually have both of those simultaneously!  That's because the PTE pointsto will assert full ownership of the physical memory cell holding the PTE as its data value, while the virtual pointsto for the data mapped by that entry will \emph{also} assert (fractional) ownership of all entries a page table walk would traverse.
%}
%\todo[inline,color=violet]{This doesn't seem to cause issues with the mapping/unmapping examples, only with changing intermediate page table pointers. The mapping example requires a virtual pointsto for the blank PTE, and once filled in that ownership can be immediately split to create the 512 new virtual pointsto assertions for the newly mapped page. Conversely, for unmapping we'd assume ownership of all the relevant virtual pointsto assertions for the page we're unmapping, at which point we can (with a bit of work) show that they all correspond to the same L1 PTE, and extract the 512 fractional shares of that entry from the pointsto assertions.  But changing intermediate page tables, as one would do for coallescing or splitting a superpage while preserving the virtual-to-physical mappings, couldn't be done without some really complicated separating implication tricks.}
%\todo[inline,color=green]{One possible approach to resolving this, which we came up with in our Tuesday meeting, is to recognize that the current (v1) virtual points-to is too strong, because it really doesn't care about \emph{owning} those fractional resources, it only cares that \emph{something} ensures the correct page table walk exists. Iris has a ghost map resource where authoritative ownership of an individual key-value pair can be handled as a resource.  (Colin was using this in the filesystem cache.)
%We can use that mechanism to separate the virtual-to-physical translation from the physical memory involved (Kolanski and Klein may have done something similar for different reasons): (fractional) virtual points-to assertions can be defined in terms of (fractional) ownership of these authoritative ghost map entry assertions, plus sharing an invariant that the current installed page table respects all entries of the mapping. Unmapping collects the authoritative map kvpairs from collecting the assertions, and then can remove them from the ghost map and update the page tables. Critically, physical ownership of the page tables then lives in the invariant on the current page table, so some virtual pointsto assertions can refer to memory in those page tables.
%This still works with the modality, since that invariant is also semantically a predicate on a page table root.
%Let's call this v2.
%}
%\end{comment}
\subsection{Mapping a New Page}
One of the key tasks of a page fault handler in a general-purpose OS kernel is
to map new pages into an address space by writing into an existing page table.
To do so, we first allocate a fresh page, then calculate the appropriate
known-valid page table walks and update the appropriate L1 page table entry.
%\lstset{
%  columns=fullflexible,
%  numbers=left,
%  basicstyle=\ttfamily,
%  keywordstyle=\color{blue}\bfseries,
%  morekeywords={mov,add,call},
%  emph={rsp,rdx,rax,rbx,rbp,rsi,rdi,rcx,r8,r9,r10,r11,r12,r13,r14,r15},
%  emphstyle=\color{green},
%  emph={[2]cr3},
%  emphstyle={[2]\color{violet}},
%  morecomment=[l]{;;},
%  mathescape
%}
\newcommand{\fpaddr}{\texttt{fpaddr}}
\newcommand{\specline}[1]{{\color{blue}\left\{#1\right\}}}
\begin{figure}\footnotesize
  \begin{lstlisting}[mathescape]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m)  \ast  \texttt{r14}\mapsto_{\textsf{r}} \_ \ast \texttt{rdi}\mapsto_{r} \vaddr \ast \texttt{rax}\mapsto_{\textsf{r}} \_ \ast \ulcorner \texttt{aligned va} \land  \theta \; !!\; \vaddr = \texttt{None}\urcorner}_{\rtv}$
call ensure_L1_page
$\specline{\exists (\entryf ,\;\entrytr,\; \entrytw,\; \entryo,\;\textsf{pte\_addr },\paddr) \; \ldotp\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \_ \ast \texttt{rdi}\mapsto_{r} \vaddr \ast \texttt{rax}\mapsto_{\textsf{r}} \textsf{ pte\_addr} \; \ast }_{\rtv}$
$\specline{ \ulcorner  \texttt{addr\_L1 }(\vaddr, \entryo) = \paddr \urcorner \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner \; \ast}_{\rtv}$
$\specline{\nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \; \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\entryo \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{pte\_addr}  }_{\rtv}$
;; Returns the virtual address of the L1 entry in rax
mov r14, rax ;; Save that before another call
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \ast \texttt{rax}\mapsto_{\textsf{r}} \textsf{ pte\_addr} \; \ast }_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\entryo \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{wzero 64}) \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{pte\_addr}  }_{\rtv}$
call alloc_phys_page_or_panic
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \;\ast}_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\naddr \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr\; (\texttt{wzero 64})  \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner}_{\rtv}$
$\specline{\exists \texttt{ fpaddr} \ldotp \ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{fpaddr+3} \ast \texttt{fpaddr} \mapsto_{\textsf{p}} (\texttt{wzero 64}) \ast \ulcorner \texttt{entry\_present (fpaddr+3)}\urcorner}_{\rtv}$
;; Calculate new L1 entry
;; update the page table entry, mapping the page
mov [r14], rax
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \;\ast}_{\rtv}$
$\specline{ \nfpointsto{\mask\vaddr\maskfour\rtv}{\mask\vaddr\maskfouroff\rtv}\entryf\qone\naddr \ast}_{\rtv}$ 
$\specline{  \nfpointsto{\mask\vaddr\maskthree\entryf}{\mask\vaddr\maskthreeoff\entryf}\entrytr\qtwo\naddr \ast \nfpointsto{\mask\vaddr\masktwo\entrytr}{\mask\vaddr\masktwooff\entrytr}\paddr\qthree\entryo \;\ast}_{\rtv}$
$\specline{\texttt{pte\_addr} \mapsto_{\texttt{vpte}} \paddr \;(\texttt{fpaddr+3}) \; \ast \ulcorner \texttt{entry\_present } \entryf \land \texttt{entry\_present } \entrytr \land  \texttt{entry\_present } \entrytw \urcorner }_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{fpaddr+3} \ast \texttt{fpaddr} \mapsto_{\textsf{p}} (\texttt{wzero 64}) \ast \ulcorner \texttt{entry\_present fpaddr+3}\urcorner}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast  \texttt{r14}\mapsto_{\textsf{r}} \texttt{pte\_addr} \ast \texttt{rdi}\mapsto_{\textsf{r}} \vaddr \ast }_{\rtv}$
$\specline{\textsf{L}_{4}\_\textsf{L}_{1}\_\textsf{PointsTo}(\vaddr,\entryf,\entrytr,\entrytw,\fpaddr+3) \ast \ulcorner \theta \;!!\;\vaddr = \texttt{None}\urcorner \; \ast}_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{rax}\mapsto_{\textsf{r}} \texttt{fpaddr+3} \ast \texttt{fpaddr} \mapsto_{\textsf{p}} (\texttt{wzero 64}) }_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast}_{\rtv}$
$\specline{\ulcorner \texttt{aligned fpaddr} \urcorner \ast \texttt{fpaddr} \mapsto_{\textsf{p}} \textsf{ wzero 64} \ast \sumapaces\rtv\delta  \ast\sumwalkabs\vaddr\qfrac\fpaddr}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast \vaddr \mapsto_{\textsf{vpte}}\; \{\qfrac\} \;\fpaddr \textsf{ wzero 64}}_{\rtv}$
$\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\; \sqsubseteq $
  $\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace} (<[\vaddr:=\texttt{fpaddr}]> \theta,m) \ast \vaddr \mapsto_{\textsf{v}}\; \{\qfrac\} \textsf{wzero 64}}_{\rtv}$
\end{lstlisting}
  \caption{Specification and proof of distilled code for mapping a new page.}
\label{fig:mapping_code}
\end{figure}
In Figure \ref{fig:mapping_code}, we see an address ($\vaddr$) currently not
mapped to a page ($\theta \; !!\; \vaddr = \texttt{None}$). Mapping a fresh
phyiscal page to back the desired virtual page first requires ensuring
the existence of a memory location of an appropriate L1 table entry,
which is realized in two pieces by the post-condition of \lstinline|ensure_L1|:
\begin{itemize}
\item physical pointsto assertions on the tables L4, L3, and L2 reaching to the
	L1 level entry (l1e) ensure higher levels of the page table exist, 
	are marked present (\textsf{entry\_present} for the relevant
		entries \textsf{l4e l3e l2e}) (Specification Lines 4-6) 
	\item a virtual \emph{PTE}
		pointsto ($\mapsto_{\textsf{vpte}}$) allows access to the memory of the L1 entry
		via virtual address \lstinline|pte_pointsto|.
		A PTE points-to is defined just like the normal virtual points-to of Figure \ref{fig:virtualpointstosharing}, except the physical address is explicit in the assertion (here, \textsf{pa})
		rather than existentially quantified:
 \[
\begin{array}{l}
    \vaddr\mapsto_{\textsf{vpte}} \;\{\textsf{q}\} \; \paddr \; \vpage : \mathsf{vProp}~\Sigma \stackrel{\triangle}{=} 
    \exists \delta\ldotp
	(\lambda \mathit{cr3val}\ldotp
	\sumapaces{\mathit{cr3val}}\delta) \ast 
  \sumwalkabs\vaddr\qfrac\paddr \ast \paddr \mapsto_{\mathsf{p}} \vpage
\end{array}
\]
		This supports rules for accessing memory
		at that virtual address, but exposing the physical location being modified
		makes this convenient to use for page table modifications, since we must ensure
		the modified address is the correct physical address that will be used as the L1 entry.
		This is also guaranteed ($ \ulcorner
		\texttt{addr\_L1}(\vaddr,\entryo) = \paddr\urcorner$).
\end{itemize}
After obtaining a virtual address \textsf{pte\_addr} in \textsf{rax} backed 
by the physical memory for the L1 entry that will be used to translate the virtual addresses
we are mapping, we save it to \textsf{r14} to be updated later in Line 9.

Then we allocate a fresh page initialized (i.e. zeroed) in Line 14 which return
an \textsf{aligned} address of fresh page (\textsf{fpaddr}) in \textsf{rax}
(Specification Line 19), plus 1 (so the return value is already a valid page table entry).

The crucial point in the proof is then actually updating the L1 entry,
via the virtual address
(\textsf{pte\_addr}) known to translate to the appropriate physical address, in our example the L1
table entry address ($\textsf{addr\_L1}(\textsf{va, l1e})$), to hold the freshly
allocated physical page address (\textsf{fpaddr}) in Line 22.
After this write, we have (Specification Line 26)
\[\texttt{pte\_addr} \mapsto_{\texttt{vpte}}  \; \paddr\; \textsf{fpaddr}  \qquad\qquad (\textsf{by the PTE variant of Rule}~\TirNameStyle{WriteToVirtMemFromReg})\]

At this point in the proof,  $\texttt{pte\_addr} \mapsto_{\texttt{vpte}}  \; \paddr\; \textsf{fpaddr})$ 
contains  full ownerhsip of the L1 table entry physical pointsto $\paddr
\mapsto_{\mathsf{p}} \textsf{fpaddr}$ . Then, we can split the full-ownership of that
physical pointsto
($\textsf{fpaddr}\mapsto_{\textsf{p}} \;\textsf{wzero 64}$) to obtain
fractional ownership on it appropriate for a single address's share of an L1 entry (q4).
Since we know $\texttt{addr\_L1 }(\vaddr,
\entryo) \mapsto_{\mathsf{p}} \{q4\} \;\textsf{fpaddr}$) from
$\mapsto_{\textsf{vpte}}$, after the split of the PTE points-to, we can
connect L4-L2 points-to assertions (in Specification Lines 24 and 25) with 
 $\texttt{fpaddr} \mapsto_{a} (\texttt{wzero 64})$ to obtain the
complete physical table-walk assertion (L4\_L1\_PointsTo($\vaddr$ l4e l3e l2e
fpaddr)) in Line 30. 

Finally, we can insert $\vaddr$ into the ghost page-table-walk summarization
map ($\theta$) to change its state from unmapped ($\ulcorner \theta
\;!!\;\vaddr = \texttt{None}\urcorner$ in Specification Line 30 and the
precondition) to mapped (Specification Line 33) using Iris' ghost-map update
($\sqsubseteq$), and construct a PTE -points-to $\vaddr
\mapsto_{\textsf{vpte}}\; \qfrac \;\fpaddr\;(\textsf{wzero 64})$ (Specification
Line 36) using $\sumwalkabs\vaddr\qfrac\fpaddr$ obtained from ghost
page-table-walk insertions, $\sumapaces\rtv\delta$ from unfolding the
definition of $\mapsto_{\textsf{vpte}}$ in Specification Line 18, and $\fpaddr
\mapsto_{\textsf{p}} \textsf{wzero 64}$ in Specification Line 34.
Finally, by existentially quantifying away \textsf{fpaddr},
we can shift the PTE points-to into a normal virtual points-to.

This proof only maps the first word of the allocated page; the generalization to the full page
is straightforward use of iterated separating conjunction to manage collections of clusters of 512 things
(slices of the L1 entry physical ownership, virtual addresses at 8 byte increments, etc.).

Unlike the only prior work verifying analogous code for mapping a new page~\cite{kolanski08vstte,kolanski09tphols}, our proof above
does \emph{not} need to unfold the definitions of derived logic (therefore reasoning directly over the operational semantics),
but instead uses derived logic rules throughout to reason about the effect of the mapping. By incorporating verification of the
\lstinline|ensure_L1| function (see Section \ref{sec:traversing}), our verification also directly handles several subtle aspects which
were axiomatized in prior work.

\subsection{Unmapping a Page}
The reverse operation, unmapping a designated page that is currently mapped,
would essentially be the reverse of
the reasoning around line 22 above: given the virtual points-to assertions for all 512
machine words of memory that the L1 entry would map,
and information about the physical location, 
full permission on the L1 entry could be obtained, allowing the construction of a
full virtual PTE pointer for it, setting to 0, and reclaiming the now-unmapped physical memory.

\subsection{Traversing Live Page Tables}
\label{sec:traversing}
The mapping operation of Figure \ref{fig:mapping_code} assumes an operation \lstinline|ensure_L1| which must traverse the page tables
in order to locate the address of the L1 entry to update --- possibly allocating pages at levels 3 and 2 in the process.
Traversing the page tables is itself challenging functionality to verify: loading the current table root from \lstinline|cr3| is straightforward
(a \lstinline|mov| instruction), however this produces the physical address of \lstinline|cr3|, not the virtual address the kernel code would use to access that memory.
This problem repeats at each level of the page table: assuming the code has \emph{somehow} read the appropriate L4 (or L3, or L2) entry, those entries again
yield physical addresses, not virtual.

Most kernels maintain an invariant on their page tables that the virtual address of any page used for a page table lives at a virtual address
whose value is a constant offset from the physical address a practice sometimes referred to as \emph{identity mapping} (even though the physical-to-virtual translation
is typically not literally the identity function, but adding a constant offset).\footnote{Some kernels do this for all physical memory on the machine, simplifying interaction
with DMA devices.}
\newcommand{\qfraczero}{\textsf{qfrac}}
\newcommand{\true}{\textsf{true}}
\tikzstyle{boxedassert_border} = [sharp corners,line width=0.2pt]
\NewDocumentCommand \boxedassertpv {O{} m o}{%
	\tikz[baseline=(m.base)]{
		%	  \node[rectangle, draw,inner sep=0.8pt,anchor=base,#1] (m) {${#2}\mathstrut$};
		\node[rectangle,inner sep=1.5pt,outer sep=0.2pt,anchor=base] (m) {${\,#2\,}\mathstrut$};
		\draw[#1,boxedassert_border] ($(m.south west)$) rectangle ($(m.north east)$);
	}\IfNoValueF{#3}{^{\,#3}}%
}
\newcommand*{\knowInvpv}[2]{\boxedassertpv{#2}[#1]}
\newcommand*{\ownGhostpv}[2]{\boxedassertpv[dash dot]{#2}[#1]}

\newcommand{\sumpv}[3]{
  \ownGhostpv\gammaPred{\authfrag{\singletonMap{#1}{(#2, #3)}}}
}

\newcommand{\pvmapping}[1]{\mathcal{A}\textsf{PVMappings}(#1)}
\newcommand{\sumapacesfull}[2]{
  \ownGhost\gammaPreds{\authfull{\singletonMap{#1}{#2}}}
}
\begin{figure*}
\footnotesize
\[
\begin{array}{l}
  \mathcal{I}\textsf{ASpace}_{\textsf{id}}(\ptablestore,\Xi,m)\stackrel{\triangle}{=}\\ \textsf{ASpace\_Lookup}(\ptablestore,\Xi,m) \ast 
  \bigast{(\vaddr, \textsf{paddr})\in \ptablestore}{\exists\;(\textsf{l4e, l3e, l2e, l1e, paddr})\ldotp \textsf{L}_{4}\_\textsf{L}_{1}\_\textsf{PointsTo}(\vaddr\textsf{, l4e, l3e, l2e, l1e, paddr})} \\
  \bigast{(\paddr) \in \Xi}{\exists\; (\textsf{qfrac, q, val,}\vaddr) \ldotp \ulcorner \vaddr = \paddr + \textsf{OFF} \urcorner \ast  \underbrace{\sumwalkabs\vaddr\qfrac\paddr }_\text{Ghost translation} \ast 
% \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad    
\underbrace{\paddr \mapsto_{\mathsf{p}}\{\textsf{qfrac}\}\; \vale}_\text{Physical location} \ast \ulcorner \textsf{qfrac} = 1 \leftrightarrow \; \lnot\textsf{entry\_present }(\vale) \urcorner }\\
  \textsf{ where } \\
   \textsf{ASpace\_Lookup}_{\textsf{id}}(\ptablestore,m) \stackrel{\triangle}{=} \lambda\textsf{ cr3val} \ldotp \; \exists \gammaPred \; \ldotp \ulcorner m \; !!\; \textsf{cr3val} = \textsf{Some } \gammaPred \urcorner \ast
   \ownGhost\gammaPred{\authfull{\ptableabswalk\ptablestore}} \ast  \ownGhostpv\gammaPred{\authfull{\pvmapping\Xi}}
  
\end{array}
\]
\vspace{-1em}
\caption{Global Address-Space Invariant in Figure \ref{fig:peraspaceinvariant} extended with a ghost map bookkeeping identity mappings }
  \label{fig:peraspaceinvariant_with_p2v_extension}
  \end{figure*}


\begin{figure}\footnotesize
\begin{lstlisting}[mathescape]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m) \ast \ulcorner \textsf{OFF} + \paddr > 0 \urcorner \ast \texttt{rbx}\mapsto_{\textsf{r}} \paddr \ast \sumapaces\rtv\delta  \ast \sumpv\paddr\qfrac\true  }_{\rtv}$
add rbx, OFF ;; shift physical address by identity mapping constant OFF
$\specline{\textsf{P} \ast \exists \qfraczero; \qfrac,\textsf{ w, val},  \texttt{rbx}\mapsto_{\textsf{r}} \paddr + \textsf{OFF} \ast \sumapaces\rtv\delta \ast \sumpv\paddr\qfrac\true \ast \vaddr \mapsto_{\texttt{vpte}} \{\textsf{q}, \textsf{qfrac}\} \paddr \;(\texttt{val})}$
$\specline{\ulcorner \vaddr = \paddr + \textsf{OFF} \urcorner \ast \ulcorner \textsf{qfrac} = 1 \leftrightarrow \; \lnot\textsf{entry\_present }(\vale) \urcorner \ast \sumwalkabs\vaddr\qfrac\paddr \ast (R \wand \mathcal{I}\texttt{ASpace}_{\textsf{id}}(\theta,\Xi,m)) }_{\rtv}$
\end{lstlisting}
\vspace{-1em}
\caption{Converting a physical address of a PTE to a virtual address (w/o instruction pointer or flag updates). $R$ abbreviates the resources
associated with \paddr\ by $\Xi$ in Figure \ref{fig:peraspaceinvariant_with_p2v_extension}.}
\label{fig:p2v}
\end{figure}

Work to verify the full page table walk is ongoing, but we have verified the critical step for a small x86-64 kernel, which is the physical-to-virtual
conversion (often appearing as a macro \texttt{P2V} in C source code). In our small kernel, as in larger kernels, \texttt{P2V} is actually just addition,
but the correctness of this simple instruction is quite subtle, as shown in Figure \ref{fig:p2v}.
Beyond a frame ($P$), the precondition requires the current address space invariant, a check that the addition will not overflow, the physical address to convert
(in \lstinline|rbx|), knowledge that the current address space is valid, and knowledge that the physical address is in a range dedicated
to page table entries (as this physical-to-virtual conversion is only guaranteed for page table memory). 
The latter uses an additional ghost set added to the per-address-space
invariant per Figure \ref{fig:peraspaceinvariant_with_p2v_extension}. The extended invariant owns, for each memory location dedicated to page tables, some fraction of
that physical memory, and also guarantees the offset is the correct calculation.
Subtly, the operation ensures the relevant table entry is readable, but the exact portion of ownership returned must be determined by inspecting the valid bit
of the value in memory --- so full ownership is returned only for unused entries.
This is a simple piece of code whose functionality is critical and whose correctness is highly non-trivial. No prior work engages with this problem.

% \citet{kolanski08vstte,kolanski09tphols} verified a single code block with their logic which was roughly Figure \ref{fig:mapping_code} for a 2-level ARM
% page table, but several critical complexities our work deals with were not addressed.
% First, beyond the limitations discussed in Section \ref{sec:overly-restrictive}, Kolanski and Klein assumed that virtual addresses
% for page tables at each level were given as parameters rather than verifying any conversion from physical addresses to virtual addresses (or even axiomatizing their lookup).
% In contrast, our verification articulates the address space invariant from which the physical-to-virtual translation can be implemented.
% Second, our proof deals with the construction of a valid virtual points-to \emph{to the PTE to update} in mapping, which Kolanski and Klein also
% assumed was given.
% \todo{some of this is really an argument for our verification being more thorough, rather than being about our logic}

% Reasoning about the page table walk in their logic would have required 
% could reason about the walk, but would need to explicitly prove that all other invariants
% of the kernel, the current address space, and all other address spaces of interest were preserved by each update, because their model
% only supports separation within a single address space. In our model, this follows for free from making
% our separation logic directly aware of address translation and internalizing assumptions about other address spaces as further separable assertions.
% Kolanski and Klein did address part of the walk information for a 2-level page table (a possible ARM configuration), but 

% \textsc{seL4} currently still trusts address translations; it models page tables as a data structure in regular memory, thus not capturing the possibility that even
% temporarily destroying the mappings and restoring them can actually crash the OS. \textsc{CertiKOS} papers share little in the way of precise details about
% their virtual memory management, but because their core technology is based on a fork of \textsc{CompCert}, whose model of memory is
% a set of unordered block allocations, we can infer their proofs must also trust these translations.


\subsection{Change of Address Space}
A critical piece of \emph{trusted} code in current verified OS kernels is the assembly code to change the current address space; current verified OS kernels currently 
lack effective ways to specify and reason about this low-level operation, for reasons outlined in Section \ref{sec:relwork}.
\begin{figure}\footnotesize
\begin{lstlisting}[mathescape]
;; Assume the save-space is in rdi, load-space in rsi. First, save the yielding context
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'](\mathcal{I}\texttt{ASpace}(\theta',m') \ast \texttt{Pother}) \ast \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \texttt{rbx}\mapsto_{\textsf{r}} \texttt{rbxv} }_{\rtv}$
$\specline{\texttt{rsp}\mapsto_{\textsf{r}} \texttt{rspv} \ast \texttt{rbp}\mapsto_{\textsf{r}} \texttt{rbpv} \ast \texttt{r12}\mapsto_{\textsf{r}} \texttt{r12v} \ast \texttt{r13}\mapsto_{\textsf{r}} \texttt{r13v} \ast \texttt{r14}\mapsto_{\textsf{r}} \texttt{r14v} \ast \texttt{r15}\mapsto_{\textsf{r}} \texttt{r15v}}_{\rtv}$
$\specline{\mathsf{ContextAt}(old,\_) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']) }_{\rtv}$
mov 0[rdi], rbx
... ;; mov rsp, rbp, r12, r13, r14, saved to offsets 8, 16, 24, 32, and 40 from rdi
mov 48[rdi], r15
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'](\mathcal{I}\texttt{ASpace}(\theta',m') \ast \texttt{Pother})  \ast  \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \texttt{rbx}\mapsto_{\textsf{r}} \texttt{rbxv} }_{\rtv}$
$\specline{ \texttt{rsp}\mapsto_{\textsf{r}} \texttt{rspv} \ast \texttt{rbp}\mapsto_{\textsf{r}} \texttt{rbpv} \ast \texttt{r12}\mapsto_{\textsf{r}} \texttt{r12v} \ast \texttt{r13}\mapsto_{\textsf{r}} \texttt{r13v} \ast \texttt{r14}\mapsto_{\textsf{r}} \texttt{r14v} \ast \texttt{r15}\mapsto_{\textsf{r}} \texttt{r15v}}_{\rtv}$
$\specline{\mathsf{ContextAt}(old,[\mathsf{rbxv,\ldots,\rtv}]) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']) }_{\rtv}$
mov 56[%rdi], %cr3
$\specline{\ldots \ast \texttt{rdi+56} \mapsto_{\textsf{v}} \rtv}_{\rtv}$    
;; Restore target context
mov rbx, 0[rsi] 
mov rsp, 8[rsi] ;; Switch to new stack, which may not be mapped in the current address space!
... ;; load rbp, r12, r13, r14, from offsets 16, 24, 32, and 40 from rsi
mov r15, 48[rsi]
$\specline{\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast [\rtv'](\mathcal{I}\texttt{ASpace}(\theta',m') \ast \texttt{Pother})  \ast  \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \texttt{rbx}\mapsto_{\textsf{r}} \texttt{rbxv}' }_{\rtv}$
$\specline{\texttt{rsp}\mapsto_{\textsf{r}} \texttt{rspv}' \ast \texttt{rbp}\mapsto_{\textsf{r}} \texttt{rbpv}' \ast \texttt{r12}\mapsto_{\textsf{r}} \texttt{r12v}' \ast \texttt{r13}\mapsto_{\textsf{r}} \texttt{r13v}' \ast \texttt{r14}\mapsto_{\textsf{r}} \texttt{r14v}' \ast \texttt{r15}\mapsto_{\textsf{r}} \texttt{r15v}'}_{\rtv}$
$\specline{\mathsf{ContextAt}(old,[\mathsf{rbxv,\ldots,\rtv}]) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']) }_{\rtv}$
;; Switch to the new address space
mov cr3, 56[rsi]
$\specline{ [\rtv](\textsf{P} \ast \mathcal{I}\texttt{ASpace}(\theta,m) \ast \mathsf{ContextAt}(old,[\mathsf{rbxv,\ldots,\rtv}]) \ast \mathsf{ContextAt}(new,[\mathsf{rbxv}',\ldots,\rtv']))  }_{\rtv}$
$\specline{ \mathcal{I}\texttt{ASpace}(\theta',m') \ast \texttt{Pother} \ast  \texttt{rsi}\mapsto_{\textsf{r}} new \ast \texttt{rdi}\mapsto_{\textsf{r}} old \ast \ldots }_{\rtv'}$
\end{lstlisting}
\vspace{-1em}
\caption{Basic task switch code that switches address spaces.}
\label{fig:swtch}
\end{figure}

Figure \ref{fig:swtch} gives simplified code for a basic task switch, the heart of an OS scheduler implementation. This is code that saves the context (registers and stack)
of the running thread (here in a structure pointed to by \lstinline|rdi|'s value shown in Lines 5--7 and Line 11 of Figure \ref{fig:swtch}) and restores the context of 
an existing thread (from \lstinline|rsi| shown in abbreviated Lines 14--17 and Line 22), including the corresponding change of address space for a target thread in another process.
This code assumes the System V AMD64 ABI calling convention, where the normal registers not mentioned are caller-save, and therefore saved on the stack of the thread
that calls this code, as well as on the new stack of the thread that is restored, thus only the callee-save registers and \texttt{cr3} must be 
restored.\footnote{We are simplifying in a couple basic ways. First, we are ignoring non-integer registers (e.g., floating point, vector registers) entirely. Second, we are ignoring that the caller-save registers should still be initialized to 0 to avoid leaking information across processes. We focus on the core logical requirements.}
With the addition of a return instruction, this code would satisfy the C function signature\footnote{The name comes from the UNIX 6th Edition \lstinline|swtch| function, the source of the infamous ``You are not expected to understand this'' comment~\cite{lions1996lions}.}
\begin{lstlisting}[language=C]
void swtch(context_t* save, context_t* restore);
\end{lstlisting}
A call to this code begins executing one thread (up through Line 17) in one address space ($\rtv$), whose information will be saved in a structure at address $old$,
and finishes execution executing a different thread in a different address space (whose information is initially in $new$).

Because this code does not directly update the instruction pointer, it is worth explaining \emph{how} this switches threads: by switching address spaces and stacks. 
This is meant to be called with a return address for the current thread stored on the current stack when called --- which must be reflected in the calling convention. 
In particular, the precondition of the return address on the initial stack requires the callee-save register values at the time of the call: those stored in the first 
half of the code.
Likewise, part of the invariant of the stack of the second thread, the one being restored, is that the return address on \emph{that} stack requires the saved 
callee-save registers stored in that context to be in registers as its precondition.

The wrinkle, and the importance of the modal treatment of assertions, is that the target thread's precondition is \emph{relative to its address space}, 
not the address space of the calling thread shown as 
\[[\rtv'](\mathcal{I}\texttt{ASpace}(\theta,m) \ast \texttt{Pother})\]
in the specfication. 
Thus the precondition of this code,
in context, would include that the initial stack pointer (before \lstinline|rsp| is updated)
has a return address expecting the then-current callee-save register values and 
suitably updated (i.e., post-return) stack in the \emph{current} (initial) address space;
this would be part of \textsf{P} in the precondition.
The specification also requires that
the stack pointer saved in the context to restore expects the same of the saved registers and stack 
\emph{in the other address space}. 
The other-space modality plays a critical role here; \textsf{Pother} would contain these assumptions in the other
address space.

% Lines 10--16 save the current context into memory (in the current address space).
% Line 22 saves the initial page table root.
% Lines 33--38 begin restoring the target context, including the stack pointer (line 33),
% which may not be mapped in the address space at that time: it is the stack for the context being
% loaded into the CPU.
% The actual address switch occurs on line 45, which is verified with our modal rule for updating \lstinline|cr3|,
% and thus shifts resources in and out of other-space modalities as appropriate.

The postcondition is analagous to the precondition, but interpreted \emph{in the new address space}: the then-current (updated) stack would have a return address expecting the new (restored) register values (again, in \textsf{Pother}),
and the saved context's invariant captures the precondition for restoring its execution \emph{in the previous address space} (as part of \textsf{P}). 

Note that immediately after the page table switch, the points-to information about the saved and restored contexts is guarded by a modality for the retiring
address space \rtv (Line 23). This is enforced by \textsc{WriteToRegCtlFromRegModal} (Figure \ref{fig:wpdamd}), and is sensible because
there is no general guarantee that the data structures of the previous addres space are mapped in the new address space.
The ability to transfer that points-to information out of that modality is specific to a given kernel's design. 
Kernels that map kernel memory into all address spaces would need to ensure and specify enough specific details about memory mappings to allow a 
proof of an elimination rule for specific modally-constrained points-to assertions.
% Following Spectre and Meltdown, this kernel design became less prevalent because speculative execution of accesses to kernel addresses could leak information even if the access did eventually cause a fault (the user/kernel mode permission check was done after fetching data from memory). Thus many modern kernels have reverted to the older kernel design where the kernel inhabits its own unique address space, and user processes have only enough extra material mapped in their address spaces to switch into the kernel (CPUs do not speculate past updates to \texttt{cr3}).

